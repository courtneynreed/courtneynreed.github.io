<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>papers | Courtney N. Reed</title> <meta name="author" content="Courtney N. Reed"> <meta name="description" content="Courtney N. Reed's "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://courtneynreed.github.io/papers/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Courtney </span>N. Reed</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item active"> <a class="nav-link" href="/papers/">papers<span class="sr-only">(current)</span></a> </li> <li class="nav-item"><a class="nav-link" href="https://docs.google.com/document/d/1zmnmAj1ok-t0i4b_8pbCEB78QDXGkr-yNywraocT8vM/edit?usp=sharing" rel="external nofollow noopener" target="_blank">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">papers</h1> <p class="post-description"></p> </header> <article> <p>For a full list of publications and citations, please see my <a href="https://scholar.google.com/citations?hl=en&amp;user=iAtFS0wAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> profile.</p> <p><em>(*combined first authorship)</em></p> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TriangleSimplex-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TriangleSimplex-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TriangleSimplex-1400.webp"></source> <img src="/assets/img/publication_preview/Solinski_CinC23_TriangleSimplex.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Solinski_CinC23_TriangleSimplex.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Solinski_CinC23_TriangleSimplex" class="col-sm-7"> <div class="title">Triangle Simplex Plots for Representing and Classifying Heart Rate Variability</div> <div class="author"> Mateusz Soliński, <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In 2023 Computing in Cardiology Conference (CinC)</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Best Poster</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Solinski_CinC23_TriangleSimplex.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Simplex plots afford barycentric mapping and visualisation of the ratio of three variables, summed to a constant, as positions in an equilateral triangle (2-simplex); for instance, time distribution in three-interval musical rhythms. We propose a novel use of simplex plots to visualise the balance of autonomic variables and classification of autonomic states during baseline and music performance. RR interval series extracted from electrocardiographic (ECG) traces were collected from a musical trio (pianist, violinist, cellist) in a baseline (5 min) and music performance (\sim10 min) condition. Schubert’s Trio Op. 100, \textitAndante con moto was performed in nine rehearsal sessions over five days. Each RR interval series’ very low (VLF), low (LF), and high (HF) frequency component power values, calculated in 30 sec windows (hop size 15 sec), were normalised to 1 and visualised in triangle simplex plots. Spectral clustering was used to cluster data points for baseline and music conditions. We correlated the accuracy between the clustered and true values. Strong negative correlation was observed for the violinist (r = –0.80, p ≤.01, accuracy range: [0.64, 0.94]) and pianist (r = –0.62, p = .073, [0.64, 0.80]), suggesting adaptation of their cardiac response (reduction between baseline and performance) over the performances; a weakly negative, non-significant correlation was observed for the cellist (r = –0.23, p = .545, [0.50, 0.61]), indicating similarity between baseline and performance over time. Using simplex plots, we were able to effectively represent VLF, LF and HF ratios and track changes in autonomic response over a series of music rehearsals to observe autonomic states and changes over time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Solinski_CinC23_TriangleSimplex</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Triangle Simplex Plots for Representing and Classifying Heart Rate Variability}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliński, Mateusz and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{2023 Computing in Cardiology Conference (CinC)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TRDanalysis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TRDanalysis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Solinski_CinC23_TRDanalysis-1400.webp"></source> <img src="/assets/img/publication_preview/Solinski_CinC23_TRDanalysis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Solinski_CinC23_TRDanalysis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Solinski_CinC23_TRDanalysis" class="col-sm-7"> <div class="title">Time Delay Stability Analysis of Pairwise Interactions Amongst Ensemble-Listener RR Intervals and Expressive Music Features</div> <div class="author"> Mateusz Soliński, <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In 2023 Computing in Cardiology Conference (CinC)</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Best Poster</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Solinski_CinC23_TRDanalysis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Time Delay Stability (TDS) can reveal physiological function and states in networked organs. Here, we introduce a novel application of TDS to a musical setting to study interactions between RR intervals of ensemble musicians and a listener, and music properties. Three musicians performed a movement from Schubert’s Trio Op. 100 nine times in the company of one listener. Their RR intervals were collected during baseline (5 min, silence) and performances (\sim10 min each). Loudness and tempo were extracted from recorded music audio. Regions of stable optimal time delay were identified during baseline and music, shuffled data, and data pairs from incongruent recordings. Bootstrapping was employed to obtain mean TDS probabilities (calculated based on all performances). A significant difference in mean TDS probability between music and baseline is observed for all musician pairs (p&lt;.001) and for cello-listener (p=.025); mean TDS probability being greater during music. A significant decrease in mean TDS probability was observed for piano-violin (p&lt;.001), violin-tempo (p=.045), and cello-tempo (p&lt;.001) for incongruent pairs. The highest inter-musician TDS probabilities were observed in musically tense sections: the final climax before the music dies down for the ending and mid piece in a suspenseful swell. This framework offers a promising way to track dynamic RR interval interactions between people engaged in a shared activity, and, in this musical activity, between the people and music properties.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Solinski_CinC23_TRDanalysis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Time Delay Stability Analysis of Pairwise Interactions Amongst Ensemble-Listener RR Intervals and Expressive Music Features}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliński, Mateusz and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{2023 Computing in Cardiology Conference (CinC)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_VocalMetaphor.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_VocalMetaphor" class="col-sm-7"> <div class="title">Negotiating Experience and Communicating Information Through Abstract Metaphor</div> <div class="author"> <em>Courtney N. Reed</em>, Paul Strohmeier, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_VocalMetaphor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>An implicit assumption in metaphor use is that it requires grounding in a familiar concept, prominently seen in the popular Desktop Metaphor. In human-to-human communication, however, abstract metaphors, without such grounding, are often used with great success. To understand when and why metaphors work, we present a case study of metaphor use in voice teaching. Voice educators must teach about subjective, sensory experiences and rely on abstract metaphor to express information about unseen and intangible processes inside the body. We present a thematic analysis of metaphor use by 12 voice teachers. We found that metaphor works not because of strong grounding in the familiar, but because of its ambiguity and flexibility, allowing shared understanding between individual lived experiences. We summarise our findings in a model of metaphor-based communication. This model can be used as an analysis tool within the existing taxonomies of metaphor in user interaction for better understanding why metaphor works in HCI. It can also be used as a design resource for thinking about metaphor use and abstracting metaphor strategies from both novel and existing designs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_VocalMetaphor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Negotiating Experience and Communicating Information Through Abstract Metaphor}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580700}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580700}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{sensory experience, design, data representations, metaphor, human communication}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_TactileSymbols-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_TactileSymbols-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_TactileSymbols-1400.webp"></source> <img src="/assets/img/publication_preview/Sabnis_CHI23_TactileSymbols.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Sabnis_CHI23_TactileSymbols.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sabnis_CHI23_TactileSymbols" class="col-sm-7"> <div class="title">Tactile Symbols with Continuous and Motion-Coupled Vibration: An Exploration of Using Embodied Experiences for Hermeneutic Design</div> <div class="author"> Nihar Sabnis, Dennis Wittchen, Gabriela Vega, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Courtney N. Reed, Paul Strohmeier' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Sabnis_CHI23_TactileSymbols.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With most digital devices, vibrotactile feedback consists of rhythmic patterns of continuous vibration. In contrast, when interacting with physical objects, we experience many of their material properties through vibration which is not continuous, but dynamically coupled to our actions. We assume the first style of vibration to lead to hermeneutic mediation, while the second style leads to embodied mediation. What if both types of mediation could be used to design tactile symbols? To investigate this, five haptic experts designed tactile symbols using continuous and motion-coupled vibration. Experts were interviewed to understand their symbols and design approach. A thematic analysis revealed themes showing that lived experience and affective qualities shaped design choices, that experts optimized for passive or active symbols, and that they considered context as part of the design. Our study suggests that adding embodied experiences as a design resource changes how participants think of tactile symbol design, thus broadening the scope of the symbol by design for context, and expanding their affective repertoire as changing the type of vibration influences perceived valence and arousal.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sabnis_CHI23_TactileSymbols</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Tactile Symbols with Continuous and Motion-Coupled Vibration: An Exploration of Using Embodied Experiences for Hermeneutic Design}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sabnis, Nihar and Wittchen, Dennis and Vega, Gabriela and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3581356}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3581356}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{688}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{tactons, postphenomenology, vibrotactile feedback, embodied interaction, symbol design}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_HapticServos-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_HapticServos-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Sabnis_CHI23_HapticServos-1400.webp"></source> <img src="/assets/img/publication_preview/Sabnis_CHI23_HapticServos.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Sabnis_CHI23_HapticServos.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sabnis_CHI23_HapticServos" class="col-sm-7"> <div class="title">Haptic Servos: Self-Contained Vibrotactile Rendering System for Creating or Augmenting Material Experiences</div> <div class="author"> <em>Courtney N. Reed*</em>, Nihar Sabnis*, Dennis Wittchen*, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Narjes Pourjafarian, Jürgen Steimle, Paul Strohmeier' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Honourable Mention Best Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Sabnis_CHI23_HapticServos.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When vibrations are synchronized with our actions, we experience them as material properties. This has been used to create virtual experiences like friction, counter-force, compliance, or torsion. Implementing such experiences is non-trivial, requiring high temporal resolution in sensing, high fidelity tactile output, and low latency. To make this style of haptic feedback more accessible to non-domain experts, we present Haptic Servos: self-contained haptic rendering devices which encapsulate all timing-critical elements. We characterize Haptic Servos’ real-time performance, showing the system latency is &lt;5 ms. We explore the subjective experiences they can evoke, highlighting that qualitatively distinct experiences can be created based on input mapping, even if stimulation parameters and algorithm remain unchanged. A workshop demonstrated that users new to Haptic Servos require approximately ten minutes to set up a basic haptic rendering system. Haptic Servos are open source, we invite others to copy and modify our design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sabnis_CHI23_HapticServos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Haptic Servos: Self-Contained Vibrotactile Rendering System for Creating or Augmenting Material Experiences}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed*, Courtney N. and Sabnis*, Nihar and Wittchen*, Dennis and Pourjafarian, Narjes and Steimle, J\"{u}rgen and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580716}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580716}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{522}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{haptic rendering, material experiences, prototyping, toolkit, haptic feedback}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Wittchen_AHs23_AugmentedShoes-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Wittchen_AHs23_AugmentedShoes-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Wittchen_AHs23_AugmentedShoes-1400.webp"></source> <img src="/assets/img/publication_preview/Wittchen_AHs23_AugmentedShoes.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Wittchen_AHs23_AugmentedShoes.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wittchen_AHs23_AugmentedShoes" class="col-sm-7"> <div class="title">Designing Interactive Shoes for Tactile Augmented Reality</div> <div class="author"> Dennis Wittchen, Valentin Martinez-Missir, Sina Mavali, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Nihar Sabnis, Courtney N. Reed, Paul Strohmeier' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2023</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Wittchen_AHs23_AugmentedShoes.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Augmented Footwear has become an increasingly common research area. However, as this is a comparatively new direction in HCI, researchers and designers are not able to build upon common platforms. We discuss the design space of shoes for augmented tactile reality, focussing on physiological and biomechanical factors as well as technical considerations. We present an open source example implementation from this space, intended as an experimental platform for vibrotactile rendering and tactile AR and provide details on experiences that could be evoked with such a system. Anecdotally, the new prototype provided experiences of material properties like compliance, as well as altered perception of their movements and agency. We intend our work to lower the barrier of entry for new researchers and to support the field of tactile rendering in footwear in general by making it easier to compare results between studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wittchen_AHs23_AugmentedShoes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Designing Interactive Shoes for Tactile Augmented Reality}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wittchen, Dennis and Martinez-Missir, Valentin and Mavali, Sina and Sabnis, Nihar and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2023}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Glasgow, United Kingdom}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '23}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3582700.3582728}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399845}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3582700.3582728}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{haptic footwear, haptic rendering, vibrotactile augmentation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI23_BodyAsSound.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI23_BodyAsSound.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI23_BodyAsSound" class="col-sm-7"> <div class="title">The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI23_BodyAsSound.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Multi-sensory experiences underpin embodiment, whether with the body itself or technological extensions of it. Vocalists experience intensely personal embodiment, as vocalisation has few outwardly visible effects and kinaesthetic sensations occur largely within the body, rather than through external touch. We explored this embodiment using a probe which sonified laryngeal muscular movements and provided novel auditory feedback to two vocalists over a month-long period. Somatic and micro-phenomenological approaches revealed that the vocalists understand their physiology through its sound, rather than awareness of the muscular actions themselves. The feedback shaped the vocalists’ perceptions of their practice and revealed a desire for reassurance about exploration of one’s body when the body-as-sound understanding was disrupted. Vocalists experienced uncertainty and doubt without affirmation of perceived correctness. This research also suggests that technology is viewed as infallible and highlights expectations that exist about its ability to dictate success, even when we desire or intend to explore.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI23_BodyAsSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Warsaw, Poland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '23}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3569009.3572738}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399777}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3569009.3572738}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{embodiment, tacit knowledge, musical interaction, auditory feedback, sensory translation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI23_BodyLutherie.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_BodyLutherie.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_BodyLutherie" class="col-sm-7"> <div class="title">As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Body X Materials</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_BodyLutherie.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through soma-centric research, we see the different interaction roles of our bodies: they are the locus of our experience, a conduit for our expression and engagement, a sensor of feedback in the world, and a collaborator in our interaction with it. More" traditional" examinations of the body might look at control over it; for instance, in my research around vocal embodiment, I see many teachers and practitioners alike talking about how we can maintain control over the body. However, bodies are living, inconsistent, and typically weird. In reality, we do not have as much control over them as we would like or think we do. In this position paper, I will touch on my research around vocal physiology and sonified and vibrotactile feedback as I frame our role in a new light—designers as Body Luthiers, who must address the body as a material with inconsistencies, flaws, and variability, and work with it as a partner, embracing its uniqueness and changeability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_BodyLutherie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Body X Materials}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME23_QueryingExperience-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME23_QueryingExperience-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME23_QueryingExperience-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME23_QueryingExperience.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME23_QueryingExperience.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME23_QueryingExperience" class="col-sm-7"> <div class="title">Querying Experience with Musical Interaction</div> <div class="author"> Courtney Reed, Eevee Zayas-Garin, and Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Workshop</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME23_QueryingExperience.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With this workshop, we aim to bring together researchers with the common interest of querying, articulating and understanding experience in the context of New Interfaces for Musical Expression, and to jointly identify challenges, methodologies and opportunities in this space. Furthermore, we hope it serves as a platform for strengthening the community of researchers working with qualitative and phenomenological methods around the design of DMIs and HCI applied to musical interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME23_QueryingExperience</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney and Zayas-Garin, Eevee and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Querying Experience with Musical Interaction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Mexico City, Mexico}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Haynes_TEI23_BeingMeaningful-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Haynes_TEI23_BeingMeaningful-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Haynes_TEI23_BeingMeaningful-1400.webp"></source> <img src="/assets/img/publication_preview/Haynes_TEI23_BeingMeaningful.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Haynes_TEI23_BeingMeaningful.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Haynes_TEI23_BeingMeaningful" class="col-sm-7"> <div class="title">Being Meaningful: Weaving Soma-Reflective Technological Mediations into the Fabric of Daily Life</div> <div class="author"> Alice Haynes, Courtney Reed, Charlotte Nordmoen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sophie Skach' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Workshop</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Haynes_TEI23_BeingMeaningful.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A one-size-fits-all design mentality, rooted in objective efficiency, is ubiquitous in our mass-production society. This can negate peoples’ experiences, bodies, and narratives. Ongoing HCI research proposes design for meaningful relations; but for many researchers, the practical implementation of these philosophies remains somewhat intangible. In this Studio, we playfully tackle this space by engaging with the nuances of soft, flexible, and organic materials, collectively designing probes to embrace plurality, embody meaning, and encourage soma-reflection. Focusing on materiality and practices from e-textiles, soft robotics, and biomaterials research, we address technology’s role as a mediator of our experiences and determiner of our realities. The processes and probes developed in this Studio will serve as an experiential manifesto, providing practitioners with tools to deepen their own practices for designing soma-reflective tangible and embodied interaction. The Studio will form the first steps for ongoing collaboration, focusing on bespoke design and curation of meaningful, personal relationships.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Haynes_TEI23_BeingMeaningful</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haynes, Alice and Reed, Courtney and Nordmoen, Charlotte and Skach, Sophie}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Being Meaningful: Weaving Soma-Reflective Technological Mediations into the Fabric of Daily Life}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399777}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3569009.3571844}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3569009.3571844}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Soma Design, Materiality, Somaesthetics, Plurality, Feminism}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Warsaw, Poland}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '23}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_PhD_ImaginingSensing.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_PhD_ImaginingSensing.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_PhD_ImaginingSensing" class="col-sm-7"> <div class="title">Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>Queen Mary University of London</em>, 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>PhD Thesis</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_PhD_ImaginingSensing.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> The voice is body and instrument. Third-person interpretation of the voice by listeners, vocal teachers, and digital agents is centred largely around audio feedback. For a vocalist, physical feedback from within the body provides an additional interaction. The vocalist’s understanding of their multi-sensory experiences is through tacit knowledge of the body. This knowledge is difficult to articulate, yet awareness and control of the body are innate. In the ever-increasing emergence of technology which quantifies or interprets physiological processes, we must remain conscious also of embodiment and human perception of these processes. Focusing on the vocalist-voice relationship, this thesis expands knowledge of human interaction and how technology influences our perception of our bodies. To unite these different perspectives in the vocal context, I draw on mixed methods from cognitive science, psychology, music information retrieval, and interactive system design. Objective methods such as vocal audio analysis provide a third-person observation. Subjective practices such as micro-phenomenology capture the experiential, first-person perspectives of the vocalists themselves. Quantitative-qualitative blend provides details not only on novel interaction, but also an understanding of how technology influences existing understanding of the body.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Reed_PhD_ImaginingSensing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Queen Mary University of London}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME22_Microphenomenology.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME22_Microphenomenology.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME22_Microphenomenology" class="col-sm-7"> <div class="title">Exploring Experiences with New Musical Instruments through Micro-phenomenology</div> <div class="author"> <em>Courtney N. Reed</em>, Charlotte Nordmoen, Andrea Martelloni, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Giacomo Lepri, Nicole Robson, Eevee Zayas-Garin, Kelsey Cotton, Lia Mice, Andrew McPherson' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME22_Microphenomenology.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five microphenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME22_Microphenomenology</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Exploring Experiences with New Musical Instruments through Micro-phenomenology}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{The University of Auckland, New Zealand}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21428/92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.21428%2F92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{49}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/-Ket6l90S8I}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_AHs22_SingingKnit.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_AHs22_SingingKnit.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_AHs22_SingingKnit" class="col-sm-7"> <div class="title">Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances</div> <div class="author"> <em>Courtney N. Reed</em>, Sophie Skach, Paul Strohmeier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew P. McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2022</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_AHs22_SingingKnit.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer’s vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_AHs22_SingingKnit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Skach, Sophie and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2022}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kashiwa, Chiba, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{170–183}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3519391.3519412}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450396325}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3519391.3519412}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{wearables, Design, singing, music performance, knit, electromyography, fabric sensors, biosignals}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_CommunicatingBodies.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_CommunicatingBodies" class="col-sm-7"> <div class="title">Communicating Across Bodies in the Voice Lesson</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Tangible Interaction for Well-Being</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_CommunicatingBodies.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this position paper, I would like to introduce my research on vocalists and their relationships with their bodies, and how the use of haptic feedback can improve these connections and the way we communicate sensory experience. I use the voice lesson and vocal performance as an environment to understand more broadly how people perceive very refined movements which they feel internally. My research seeks to understand how we communicate these sensory experiences in human-to-human interaction and how we can augment or communicate sensory experience through technology. I examine perception of these experiences through different feedback modalities, namely auditory and haptic feedback. Providing new ways to communicate our sensory experiences can lead to improvements in understanding between two individuals (for instance teacher and student). In virtual singing lessons, where the majority of voice study is being done in early 2022, this is especially important, as many of the common ways of interacting with the voice have disappeared with the transition to online interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_CommunicatingBodies</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Communicating Across Bodies in the Voice Lesson}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Tangible Interaction for Well-Being}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI22_SensorySketching.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_SensorySketching.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_SensorySketching" class="col-sm-7"> <div class="title">Sensory Sketching for Singers</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Sketching Across the Senses</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_SensorySketching.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This position paper outlines my study of vocalists and the relationships with the voice as both instrument and part of the body. I study this embodiment through a phenomenological perspective, employing somaesthetics and micro-phenomenology to explore the tacit relationships that singers have with their body. While verbal metaphor is traditionally used to articulate experience in teaching voice, I also use body mapping and material speculation to help articulate tactile and auditory experiences while singing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_SensorySketching</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sensory Sketching for Singers}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Sketching Across the Senses}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI22_EmbodiedSingingDC.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI22_EmbodiedSingingDC" class="col-sm-7"> <div class="title">Examining Embodied Sensation and Perception in Singing</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI22_EmbodiedSingingDC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces my PhD research on the relationship which vocalists have with their voice. The voice, both instrument and body, provides a unique perspective to examine embodied practice. The interaction with the voice is largely without a physical interface and it is difficult to describe the sensation of singing; however, voice pedagogy has been successful at using metaphor to communicate sensory experience between student and teacher. I examine the voice through several different perspectives, including experiential, physiological, and communicative interactions, and explore how we convey sensations in voice pedagogy and how perception of the body is shaped through experience living in it. Further, through externalising internal movement using sonified surface electromyography, I aim to give presence to aspects of vocal movement which have become subconscious or automatic. The findings of this PhD will provide understanding of how we perceive the experience of living within the body and perform through using the body as an instrument.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI22_EmbodiedSingingDC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining Embodied Sensation and Perception in Singing}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daejeon, Republic of Korea}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '22}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3490149.3503581}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450391474}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3490149.3503581}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Embodied interaction, lived experience, knowledge transfer, biosignals, movement perception}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI22_Vibrotouch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI22_Vibrotouch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI22_Vibrotouch-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI22_Vibrotouch.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI22_Vibrotouch.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI22_Vibrotouch" class="col-sm-7"> <div class="title">Vibro-Touch</div> <div class="author"> <em>Courtney N. Reed</em>, and Nihar Sabnis</div> <div class="periodical"> <em>In ACM TEI Studio How Tangible is TEI? Exploring Swatches as a New Academic Publication Format</em>, 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI22_Vibrotouch.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In our research, we examine tactile representations which are used for user interaction and system notifications. This swatch works as an interface to store and playback vibrotactile stimuli. This allows for easy, cost effective (\~€20) reproduction and exploration of tactile feedback. Typically, feedback designed and used in research is described through written formats. This swatch provides a companion to physically experience the vibrations. The tactile sensation is stored on a microcontroller and played back through a speaker which works as an actuator. The swatch could be given to others to test and experience different sensations in a simplified, modular format. For instance, rather than redesigning feedback for each new study, the tactile feedback could be shared and reproduced for new research. The code on the microcontroller can be changed or updated to have multiple "swatches" in one. In other applications, the swatch could also play audio feedback.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI22_Vibrotouch</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Vibro-Touch}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Sabnis, Nihar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM TEI Studio How Tangible is TEI? Exploring Swatches as a New Academic Publication Format}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daejeon, Republic of Korea}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/BryanKinns_NeurIPS_XAImusic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/BryanKinns_NeurIPS_XAImusic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/BryanKinns_NeurIPS_XAImusic-1400.webp"></source> <img src="/assets/img/publication_preview/BryanKinns_NeurIPS_XAImusic.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="BryanKinns_NeurIPS_XAImusic.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BryanKinns_NeurIPS_XAImusic" class="col-sm-7"> <div class="title">Exploring XAI for the Arts: Explaining Latent Space in Generative Music</div> <div class="author"> Nick Bryan-Kinns, Berker Banar, Corey Ford, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Courtney N. Reed, Yixiao Zhang, Simon Colton, Jack Armitage' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In 1st Workshop on eXplainable AI Approaches for Debugging and Diagnosis (XAI4Debugging@NeurIPS2021)</em>, 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/BryanKinns_NeurIPS_XAImusic.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Explainable AI has the potential to support more interactive and fluid co-creative AI systems which can creatively collaborate with people. To do this, creative AI models need to be amenable to debugging by offering eXplainable AI (XAI) features which are inspectable, understandable, and modifiable. However, currently there is very little XAI for the arts. In this work, we demonstrate how a latent variable model for music generation can be made more explainable; specifically we extend MeasureVAE which generates measures of music. We increase the explainability of the model by: i) using latent space regularisation to force some specific dimensions of the latent space to map to meaningful musical attributes, ii) providing a user interface feedback loop to allow people to adjust dimensions of the latent space and observe the results of these changes in real-time, iii) providing a visualisation of the musical attributes in the latent space to help people understand and predict the effect of changes to latent space dimensions. We suggest that in doing so we bridge the gap between the latent space and the generated musical outcomes in a meaningful way which makes the model and its outputs more explainable and more debuggable.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">BryanKinns_NeurIPS_XAImusic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Exploring XAI for the Arts: Explaining Latent Space in Generative Music}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bryan-Kinns, Nick and Banar, Berker and Ford, Corey and Reed, Courtney N. and Zhang, Yixiao and Colton, Simon and Armitage, Jack}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{1st Workshop on eXplainable AI Approaches for Debugging and Diagnosis (XAI4Debugging@NeurIPS2021)}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2308.05496}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI21_sEMGPerformance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI21_sEMGPerformance" class="col-sm-7"> <div class="title">Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI21_sEMGPerformance.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI21_sEMGPerformance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Salzburg, Austria}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '21}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3430524.3440641}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450382137}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3430524.3440641}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Biofeedback, Electromyography, First-Person Perspectives, Mental Imagery, Performer Intention}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Yang_TAFFC_emotionperception-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Yang_TAFFC_emotionperception-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Yang_TAFFC_emotionperception-1400.webp"></source> <img src="/assets/img/publication_preview/Yang_TAFFC_emotionperception.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Yang_TAFFC_emotionperception.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yang_TAFFC_emotionperception" class="col-sm-7"> <div class="title">Examining Emotion Perception Agreement in Live Music Performance</div> <div class="author"> Simin Yang, <em>Courtney N. Reed</em>, Elaine Chew, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Mathieu Barthet' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Yang_TAFFC_emotionperception.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Current music emotion recognition (MER) systems rely on emotion data averaged across listeners and over time to infer the emotion expressed by a musical piece, often neglecting time- and listener-dependent factors. These limitations can restrict the efficacy of MER systems and cause misjudgements. We present two exploratory studies on music emotion perception. First, in a live music concert setting, fifteen audience members annotated perceived emotion in the valence-arousal space over time using a mobile application. Analyses of inter-rater reliability yielded widely varying levels of agreement in the perceived emotions. A follow-up lab-based study to uncover the reasons for such variability was conducted, where twenty-one participants annotated their perceived emotions whilst viewing and listening to a video recording of the original performance and offered open-ended explanations. Thematic analysis revealed salient features and interpretations that help describe the cognitive processes underlying music emotion perception. Some of the results confirm known findings of music perception and MER studies. Novel findings highlight the importance of less frequently discussed musical attributes, such as musical structure, performer expression, and stage setting, as perceived across audio and visual modalities. Musicians are found to attribute emotion change to musical harmony, structure, and performance technique more than non-musicians. We suggest that accounting for such listener-informed music features can benefit MER in helping to address variability in emotion perception by providing reasons for listener similarities and idiosyncrasies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Yang_TAFFC_emotionperception</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining Emotion Perception Agreement in Live Music Performance}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Simin and Reed, Courtney N. and Chew, Elaine and Barthet, Mathieu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Affective Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1442--1460}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TAFFC.2021.3093787}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME20_VocalsEMG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME20_VocalsEMG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME20_VocalsEMG" class="col-sm-7"> <div class="title">Surface Electromyography for Direct Vocal Control</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, 2020 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME20_VocalsEMG.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a new method for direct control using the voice via measurement of vocal muscular activation with surface electromyography (sEMG). Digital musical interfaces based on the voice have typically used indirect control, in which features extracted from audio signals control the parameters of sound generation, for example in audio to MIDI controllers. By contrast, focusing on the musculature of the singing voice allows direct muscular control, or alternatively, combined direct and indirect control in an augmented vocal instrument. In this way we aim to both preserve the intimate relationship a vocalist has with their instrument and key timbral and stylistic characteristics of the voice while expanding its sonic capabilities. This paper discusses other digital instruments which effectively utilise a combination of indirect and direct control as well as a history of controllers involving the voice. Subsequently, a new method of direct control from physiological aspects of singing through sEMG and its capabilities are discussed. Future developments of the system are further outlined along with usage in performance studies, interactive live vocal performance, and educational and practice tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME20_VocalsEMG</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Direct Vocal Control}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Birmingham City University}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Birmingham, UK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{458--463}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.4813475}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nime.org/proceedings/2020/nime2020_paper88.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Michon, Romain and Schroeder, Franziska}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/1nWLgQGNh0g}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_ISMIR19_FeltTension-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_ISMIR19_FeltTension-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_ISMIR19_FeltTension-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_ISMIR19_FeltTension.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_ISMIR19_FeltTension.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_ISMIR19_FeltTension" class="col-sm-7"> <div class="title">Effects of Musical Stimulus Habituation and Musical Training on Felt Tension</div> <div class="author"> <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In Late Breaking/Demo at the Twentieth International Society for Music Information Retrieval Conference (ISMIR)</em>, 2019 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_ISMIR19_FeltTension.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>As a listener habituates to a stimulus, its impact is expected to decrease over time; this research investigates the impact of repetitions and time on felt tension. Farbood describes tension increase as “a feeling of rising intensity or impending climax” and decrease as “a feeling of relaxation or resolution.” Musical tension has been linked to structural properties of music such as chord movements, tonality, and section boundaries; these connections have in turn informed the design of quantitative models for musical tension. In a pilot study, 9 participants annotated their felt tension through a recorded live performance of Chopin’s Ballade No. 2 in F Maj, Op. 38, and a collage of the ballade from the Arrhythmia Suite by Chew et al. The piece includes a calm triplet motif and a tense foil with frequent dissonance and variable features. The difference in felt tension over time for musicians and non-musicians is found to be significant at 5%. In a comparison of means t-test, H0 : μ1 = μ2 is rejected at 7 DOF (t = −2.580, p = 0.0365). In musicians, the range of annotated values is greater (68.46, non-musicians = 61.75), while mean tension over time is lower (22.33, non-musicians = 35.24), suggesting that musicians are more aware of the full emotional range of their felt tension and supports the heightened response of musicians to different musical stimuli.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_ISMIR19_FeltTension</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Effects of Musical Stimulus Habituation and Musical Training on Felt Tension}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Late Breaking/Demo at the Twentieth International Society for Music Information Retrieval Conference (ISMIR)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Delft, The Netherlands}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_MSc_Changepoints-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_MSc_Changepoints-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_MSc_Changepoints-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_MSc_Changepoints.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_MSc_Changepoints.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_MSc_Changepoints" class="col-sm-7"> <div class="title">Interactions between felt emotion and musical change points in live music performance</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>Queen Mary University of London</em>, 2018 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>MSc Thesis</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_MSc_Changepoints.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> This thesis in music cognition focuses on investigations of the felt emotions that arise through a listener‘s response to musical change. The research conducted aims to connect felt emotional response to tension and pleasure in musical signatures, with a focus on qualitative categorization of participant responses. Participants indicated their felt emotional responses to three recorded pieces of music previously performed for them in a live setting. The focus of the research is on felt emotion, which is relatively unexplored compared to perceived emotion. Participants annotated their overall emotion (in valence-arousal dimensions) and tension through the music. They also annotated where they felt transition points and sudden changes to the overall emotional quality of the piece occurred. Interactions between felt emotions and musical features, including loudness, tempo, and harmonic and melodic tension, were defined over the length of the performance in both a pedagogical examination and quantitative analysis with the use of music information retrieval software. The conclusions of this study will provide a basis for future music cognition study, especially in medical research of electrophysiological effects of mental stress on the heart and brain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">Reed_MSc_Changepoints</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interactions between felt emotion and musical change points in live music performance}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Queen Mary University of London}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Courtney N. Reed. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 30, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>