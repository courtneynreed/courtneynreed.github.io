<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>papers | Courtney N. Reed</title> <meta name="author" content="Courtney N. Reed"> <meta name="description" content="Courtney N. Reed's personal/research site. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://courtneynreed.github.io/papers/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Courtney </span>N. Reed</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item active"> <a class="nav-link" href="/papers/">papers<span class="sr-only">(current)</span></a> </li> <li class="nav-item"><a class="nav-link" href="https://docs.google.com/document/d/1zmnmAj1ok-t0i4b_8pbCEB78QDXGkr-yNywraocT8vM/edit?usp=sharing" rel="external nofollow noopener" target="_blank">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">papers</h1> <p class="post-description"></p> </header> <article> <p>For a full list of publications with citations, please see my <a href="https://scholar.google.com/citations?hl=en&amp;user=iAtFS0wAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> profile.</p> <p><em>(* denotes combined first authorship)</em></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Saitis_NIME25_DeconsTimbre.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Saitis_NIME25_DeconsTimbre.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Saitis_NIME25_DeconsTimbre" class="col-sm-7"> <div class="title">(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design</div> <div class="author"> Charalampos Saitis, <em>Courtney N. Reed</em>, Ashley Noel-Hirst, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Giacomo Lepri, Andrew McPherson' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Saitis_NIME25_DeconsTimbre.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Timbre, pitch, and timing are often relevant in digital musical instrument (DMI) design. Compared with the latter two, timbre is neither easy to define nor discretise when negotiating audio representations and gesture-sound mappings. We conduct a corpus assisted discourse analysis of "timbre" in all NIME proceedings to date (2001-2024). Combining this with a detailed review of 18 timbre-focused papers at NIME, we examine how definitions of timbre and timbre interaction methods are constructed through, for instance, Wessel’s numerical timbre control space, synthesis tools and programming languages, machine learning and AI approaches, and other trends in digital lutherie practices. While acknowledging the practical utility of technical constructions of timbre in NIME (and other digital music research communities), we contribute discussion on the entanglement of technology and aesthetics in instrument design, which constitutes what "timbre" becomes in NIME research and reflect on the tension between technoscientific and constructivist understandings of timbre: how DMIs and musical practices have been reconstituted around particular timbral values operationalised in NIME. In response, we propose ways that the NIME community can embrace more critical approaches and awareness to how our methods and tools shape and co-create our notions of timbre, as well as other musical concepts, connecting more openly with diverse types of sonic phenomena.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Saitis_NIME25_DeconsTimbre</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saitis, Charalampos and Reed, Courtney N. and Noel-Hirst, Ashley and Lepri, Giacomo and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Canberra, Australia}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{197--206}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://nime.org/proceedings/2025/nime2025_29.pdf}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.15698835}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Cavdir, Doga Buse and Berthaut, Florent}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/MartinezAvila_NIME2025_Soma.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="MartinezAvila_NIME2025_Soma.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="MartinezAvila_NIME2025_Soma" class="col-sm-7"> <div class="title">Somatic and somaesthetic design practices in NIME</div> <div class="author"> Juan Martinez Avila, Doga Cavdir, <em>Courtney N. Reed</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Mary Mainsbridge, Kelsey Cotton, Tove Grimstad Bang, Lucia Montesinos Garcia' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/MartinezAvila_NIME2025_Soma.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Somatic and somaesthetic perspectives have increasingly become a part of the NIME community in recent years. In particular, many practitioners within NIME (and beyond) have adopted techniques, concepts, and tools from soma design—a methodology that centers the focus of design on the soma, i.e., the body, the lived experience, and first-person perspectives on embodied phenomena. In this workshop, we aim to gather with artists, designers, researchers, musicians, and creative practitioners at NIME to provide them with a platform to: (1) learn about soma design in NIME and musicking through our two keynotes, and group of expert panelists, (2) share their experiences of employing somatic/somaesthetic techniques to guide their creative choices, and how they are producing knowledge in the form of methods, tools, and concepts in this space, and (3) network with other like-minded researchers and practitioners who share a common body-centric perspective on design, interaction, and performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MartinezAvila_NIME2025_Soma</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Somatic and somaesthetic design practices in NIME}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Martinez Avila, Juan and Cavdir, Doga and Reed, Courtney N. and Mainsbridge, Mary and Cotton, Kelsey and Grimstad Bang, Tove and Montesinos Garcia, Lucia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Canberra, Australia}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Yarmand_CHI25_DialogicMetaphor.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Yarmand_CHI25_DialogicMetaphor.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yarmand_CHI25_DialogicMetaphor" class="col-sm-7"> <div class="title">Towards Dialogic and On-Demand Metaphors for Interdisciplinary Reading</div> <div class="author"> Matin Yarmand, <em>Courtney N. Reed</em>, Udayan Tandon, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Eric B. Hekler, Nadir Weibel, April Yi Wang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Yarmand_CHI25_DialogicMetaphor.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The interdisciplinary field of Human-Computer Interaction (HCI) thrives on productive engagement with different domains, yet this engagement often breaks due to idiosyncratic writing styles and unfamiliar concepts. Inspired by the dialogic model of abstract metaphors, as well as the potential of Large Language Models (LLMs) to produce on-demand support, we investigate the use of metaphors to facilitate engagement between Science and Technology Studies (STS) and System HCI. Our reflective-style survey with early-career HCI researchers (N=48) reported that limited prior exposure to STS research can hinder perceived openness of the work, and ultimately interest in reading. The survey also revealed that metaphors enhance likelihood to continue reading STS papers, and alternative perspectives can build critical thinking skills to mitigate potential risks of LLM-generated metaphors. We lastly offer a specified model of metaphor exchange (within this generative context) that incorporates alternative perspectives to construct shared understanding in interdisciplinary engagement.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Yarmand_CHI25_DialogicMetaphor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Towards Dialogic and On-Demand Metaphors for Interdisciplinary Reading}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yarmand, Matin and Reed, Courtney N. and Tandon, Udayan and Hekler, Eric B. and Weibel, Nadir and Wang, April Yi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Yokohama, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '25}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3706598.3713698}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{97984007139412504}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3706598.3713698}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Strohmeier_CHI25_SensorimotorDevices.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Strohmeier_CHI25_SensorimotorDevices.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Strohmeier_CHI25_SensorimotorDevices" class="col-sm-7"> <div class="title">Sensorimotor Devices: Coupling Sensing and Actuation to Augment Bodily Experience</div> <div class="author"> Paul Strohmeier, Laia Turmo Vidal, Gabriela Vega, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Courtney N. Reed, Alex Mazursky, Easa AliAbbasi, Ana Tajadura-Jiménez, Jürgen Steimle' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Strohmeier_CHI25_SensorimotorDevices.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>An emerging space in interface research is wearable devices that closely couple their sensing and actuation abilities. A well-known example is MetaLimbs [39], where sensed movements of the foot are directly mapped to the actuation of supernumerary robotic limbs. These systems are different from wearables focused on sensing, such as fitness trackers, or wearables focused on actuation, such as VR headsets. They are characterized by tight coupling between the user’s action and the resulting digital feedback from the device, in time, space, and mode. The properties of this coupling are critical for the user’s experience, including the user’s sense of agency, body ownership, and experience of the surrounding world. Understanding such systems is an open challenge, which requires knowledge not only of computer science and HCI, but also Psychology, Physiology, Design, Engineering, Cognitive Neuroscience, and Control Theory. This workshop aims to foster discussion between these diverse disciplines and to identify links and synergies in their work, ultimately developing a common understanding of future research directions for systems that intrinsically couple sensing and action.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Strohmeier_CHI25_SensorimotorDevices</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sensorimotor Devices: Coupling Sensing and Actuation to Augment Bodily Experience}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Strohmeier, Paul and Turmo Vidal, Laia and Vega, Gabriela and Reed, Courtney N. and Mazursky, Alex and AliAbbasi, Easa and Tajadura-Jiménez, Ana and Steimle, Jürgen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Yokohama, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '25}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3706599.3706735}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3706599.3706735}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Bell_TEI25_DataDialogues.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Bell_TEI25_DataDialogues.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bell_TEI25_DataDialogues" class="col-sm-7"> <div class="title">Sensory Data Dialogues: A Somaesthetic Exploration of Bordeaux through Five Senses</div> <div class="author"> Fiona Bell, Karen Anne Cochrane, Alice C Haynes, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Courtney N. Reed, Alexandra Teixeira Riggs, Marion Koelle, Laia Turmo Vidal, L. Vineetha Rallabandi' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Bell_TEI25_DataDialogues.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The design of interactive systems and digital artefacts often makes use of digital or analog sensory data as a way to "capture" human senses and sensory experiences. Yet, designing for and with sensory data is complex because of our unique, embodied ways of making sense of our somatosensory experiences. Sensory data does not have one prescribed meaning for everyone. We propose a one-day Studio at TEI to start a dialogue about work with sensory data and its representation of human sensory experience. Specifically, we propose a guided walk and series of sensory explorations in Bordeaux to contemplate the interplay between first-person somatosensory experiences and streams of site-specific data from various sensors. By walking and noticing together, this Studio invites participants to engage in a process of creative reflection on their felt experiences, their connection to their surroundings, and their stance within or outside the design community.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Bell_TEI25_DataDialogues</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sensory Data Dialogues: A Somaesthetic Exploration of Bordeaux through Five Senses}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bell, Fiona and Cochrane, Karen Anne and Haynes, Alice C and Reed, Courtney N. and Teixeira Riggs, Alexandra and Koelle, Marion and Turmo Vidal, Laia and Rallabandi, L. Vineetha}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Bordeaux / Talence, France}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '25}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3689050.3708327}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3689050.3708327}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_TOCHI_ShiftingAmbiguity.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TOCHI_ShiftingAmbiguity.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TOCHI_ShiftingAmbiguity" class="col-sm-7"> <div class="title">Shifting Ambiguity, Collapsing Indeterminacy: Designing with Data as Baradian Apparatus</div> <div class="author"> <em>Courtney N. Reed</em>, Adan L. Benito, Franco Caspe, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew P. McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM Transactions on Computer-Human Interaction</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Journal Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TOCHI_ShiftingAmbiguity.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This article examines how digital systems designers distil the messiness and ambiguity of the world into concrete data that can be processed by computing systems. Using Karen Barad’s agential realism as a guide, we explore how data is fundamentally entangled with the tools and theories of its measurement. We examine data-enabled artefacts acting as Baradian apparatuses: they do not exist independently of the phenomenon they seek to measure but rather collect and co-produce observations from within their entangled state: the phenomenon and the apparatus co-constitute one another. Connecting Barad’s quantum view of indeterminacy to the prevailing HCI discourse on the opportunities and challenges of ambiguity, we suggest that the very act of trying to stabilise a conceptual interpretation of data within an artefact has the paradoxical effect of amplifying and shifting ambiguity in interaction. We illustrate these ideas through three case studies from our own practices of designing digital musical instruments (DMIs). DMIs necessarily encode symbolic and music-theoretical knowledge as part of their internal operation, even as conceptual knowledge is not their intended outcome. In each case, we explore the nature of the apparatus, what phenomena it co-produces, and where the ambiguity lies to suggest approaches for design using these abstract theoretical frameworks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Reed_TOCHI_ShiftingAmbiguity</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Benito, Adan L. and Caspe, Franco and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Shifting Ambiguity, Collapsing Indeterminacy: Designing with Data as Baradian Apparatus}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1073-0516}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3689043}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3689043}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Computer-Human Interaction}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{73}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_CHIME24_VocalEnculturation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHIME24_VocalEnculturation.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHIME24_VocalEnculturation" class="col-sm-7"> <div class="title">Enculturation and Value Encoding in the Design of Vocal DMIs</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Proceedings of the CHIME Annual Conference 2024</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHIME24_VocalEnculturation.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper considers enculturation in vocal DMI design. Two points of inquiry propose further examination of how values and assumptions from HCI and musical practices are encoded in the design of DMIs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHIME24_VocalEnculturation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Enculturation and Value Encoding in the Design of Vocal DMIs}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the CHIME Annual Conference 2024}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{The Open University, Milton Keynes, United Kingdom}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHIME '24}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://static1.squarespace.com/static/6227c31a43daf21135453605/t/6738d984449bce18fd86195d/1731778948801/11+Courtney+N.+Reed.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Saitis_CHIME24_TimbreEthno.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Saitis_CHIME24_TimbreEthno.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Saitis_CHIME24_TimbreEthno" class="col-sm-7"> <div class="title">Ethnographic Exploration of Timbre in Hackathon Designs</div> <div class="author"> Charalampos Saitis, Bleiz Macsen Del Sette, Jordan Shier, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Haokun Tian, Shuoyang Zheng, Sophie Skach, Courtney N. Reed, Corey Ford' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the CHIME Annual Conference 2024</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Saitis_CHIME24_TimbreEthno.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper reports a summary account of the Timbre Tools Hackathon: a hackathon that invited audio developers and music technologists to consider and work with timbre through the design of tools that promote a timbre-first approach to digital instrument craft practice—timbre tools. Through ethnographic observation, we identified different approaches towards integrating timbre as an active part of creating tools and technologies in music. These strategies inform future work and the development of tools to assist awareness and exploration of timbre for instrument makers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Saitis_CHIME24_TimbreEthno</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Ethnographic Exploration of Timbre in Hackathon Designs}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saitis, Charalampos and Del Sette, Bleiz Macsen and Shier, Jordan and Tian, Haokun and Zheng, Shuoyang and Skach, Sophie and Reed, Courtney N. and Ford, Corey}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the CHIME Annual Conference 2024}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{The Open University, Milton Keynes, United Kingdom}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHIME '24}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://static1.squarespace.com/static/6227c31a43daf21135453605/t/6734b6d5b882961f8ec04316/1731507925478/6+Charalampos+Saitis+et+al.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Saitis_AM24_TimbreTools.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Saitis_AM24_TimbreTools.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Saitis_AM24_TimbreTools" class="col-sm-7"> <div class="title">Timbre Tools: Ethnographic Perspectives on Timbre and Sonic Cultures in Hackathon Designs</div> <div class="author"> Charalampos Saitis, Bleiz Macsen Del Sette, Jordan Shier, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Haokun Tian, Shuoyang Zheng, Sophie Skach, Courtney N. Reed, Corey Ford' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Saitis_AM24_TimbreTools.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Timbre is a nuanced yet abstractly defined concept. Its inherently subjective qualities make it challenging to design and work with. In this paper, we propose to explore the conceptualisation and negotiation of timbre within the creative practice of timbre tool makers. To this end, we hosted a hackathon event and performed an ethnographic study to explore how participants engaged with the notion of timbre and how their conception of timbre was shaped through social interactions and technological encounters. We present individual descriptions of each team’s design process and reflect on our data to identify commonalities in the ways that timbre is understood and informed by sound technologies and their surrounding sonic cultures, e.g., by relating concepts of timbre to metaphors. We further current understanding by offering novel interdisciplinary and multimodal insights into understandings of timbre.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Saitis_AM24_TimbreTools</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Saitis, Charalampos and Del Sette, Bleiz Macsen and Shier, Jordan and Tian, Haokun and Zheng, Shuoyang and Skach, Sophie and Reed, Courtney N. and Ford, Corey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Timbre Tools: Ethnographic Perspectives on Timbre and Sonic Cultures in Hackathon Designs}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400709685}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3678299.3678322}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3678299.3678322}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{229–244}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Milan, Italy}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AM '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Solinski_FPSYG_RRints.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Solinski_FPSYG_RRints.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Solinski_FPSYG_RRints" class="col-sm-7"> <div class="title">A framework for modeling performers’ beat-to-beat heart intervals using music features and Interpretation Maps</div> <div class="author"> Mateusz Soliński, <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>Frontiers in Psychology</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Journal Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Solinski_FPSYG_RRints.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Objective: Music strongly modulates our autonomic nervous system. This modulation is evident in musicians’ beat-to-beat heart (RR) intervals, a marker of heart rate variability (HRV), and can be related to music features and structures. We present a novel approach to modeling musicians’ RR interval variations, analyzing detailed components within a music piece to extract continuous music features and annotations of musicians’ performance decisions. Methods: A professional ensemble (violinist, cellist, and pianist) performs Schubert’s Trio No. 2, Op. 100, Andante con moto nine times during rehearsals. RR interval series are collected from each musician using wireless ECG sensors. Linear mixed models are used to predict their RR intervals based on music features (tempo, loudness, note density), interpretive choices (Interpretation Map), and a starting factor. Results: The models explain approximately half of the variability of the RR interval series for all musicians, with R-squared = 0.606 (violinist), 0.494 (cellist), and 0.540 (pianist). The features with the strongest predictive values were loudness, climax, moment of concern, and starting factor. Conclusions: The method revealed the relative effects of different music features on autonomic response. For the first time, we show a strong link between an interpretation map and RR interval changes. Modeling autonomic response to music stimuli is important for developing medical and non-medical interventions. Our models can serve as a framework for estimating performers’ physiological reactions using only music information that could also apply to listeners.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Solinski_FPSYG_RRints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{A framework for modeling performers’ beat-to-beat heart intervals using music features and Interpretation Maps}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1403599}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1664-1078}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.3389/fpsyg.2024.1403599}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fpsyg.2024.1403599}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Psychology}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Frontiers Media SA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliński, Mateusz and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Freire_NIME24_BodyLutherie.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Freire_NIME24_BodyLutherie.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Freire_NIME24_BodyLutherie" class="col-sm-7"> <div class="title">Body Lutherie: Co-Designing a Wearable for Vocal Performance with a Changing Body</div> <div class="author"> Rachel Freire, and <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Freire_NIME24_BodyLutherie.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Research at NIME has incorporated embodied perspectives from design and HCI communities to explore how instruments and performers shape each other in interaction. Material perspectives also reveal other more-than-human factors’ influence on musical interaction. We propose an additional, currently unaddressed perspective in instrument design: the influence of the body not only the locus of experience, but as a physical, entangled aspect in the more-thanhuman musicking. Proposing a practice of “Body Lutherie”, we explore how digital instrument designers can honour and work with living, dynamic bodies. Our design of a breathbased vocal wearable instrument incorporated uncontrollable aspects of a vocalist’s body and its physical change over different timescales. We distinguish the body in the design process and acknowledge its agency in vocal instrument design. Reflection on our co-design process between vocal pedagogy and eTextile fashion perspectives demonstrates how Body Lutherie can generate empathy and understanding of the body as a collaborator in future instrument design and artistic practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Freire_NIME24_BodyLutherie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Body Lutherie: Co-Designing a Wearable for Vocal Performance with a Changing Body}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Freire, Rachel and Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Utrecht, The Netherlands}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{117--126}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://nime.org/proceedings/2024/nime2024_18.pdf}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.13904800}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bin, S M Astrid and Reed, Courtney N.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/BryanKinns_CRCPress_XAI.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="BryanKinns_CRCPress_XAI.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BryanKinns_CRCPress_XAI" class="col-sm-7"> <div class="title">Explainable AI and Music</div> <div class="author"> Nick Bryan-Kinns, Berker Banar, Corey Ford, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Courtney N. Reed, Yixiao Zhang, Jack Armitage' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Artificial Intelligence for Art Creation and Understanding</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>Book Chapter</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/BryanKinns_CRCPress_XAI.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The field of eXplainable Artificial Intelligence (XAI) has become a hot topic examining how machine learning models such as neural nets and deep learning techniques can be made more understandable to humans. However, there is very little research on XAI for the arts. This chapter explores what XAI might mean for AI and art creation by exploring the potential of XAI for music generation. One hundred AI and music papers are reviewed to illustrate how AI models are being explained, or more often not explained, and to suggest some ways in which we might design XAI systems to better help humans to get an understanding of what an AI model is doing when it generates music. Then the chapter demonstrates how a latent space model for music generation can be made more explainable by extending the MeasureVAE architecture to include explainable attributes in combination with offering real-time music generation. The chapter concludes with four key challenges for XAI for music and the arts more generally: i) the nature of explanation; ii) the effect of AI models, features, and training sets on explanation; iii) user centred design of XAI; iv) Interaction Design of explainable interfaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">BryanKinns_CRCPress_XAI</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Explainable AI and Music}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bryan-Kinns, Nick and Banar, Berker and Ford, Corey and Reed, Courtney N. and Zhang, Yixiao and Armitage, Jack}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Artificial Intelligence for Art Creation and Understanding}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{CRC Press}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–29}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1201/9781003406273-1}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781003406273}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1201/9781003406273-1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_DIS24_SonicEntanglements.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_DIS24_SonicEntanglements.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_DIS24_SonicEntanglements" class="col-sm-7"> <div class="title">Sonic Entanglements with Electromyography: Between Bodies, Signals, and Representations</div> <div class="author"> <em>Courtney N. Reed</em>, Landon Morrison, Andrew P. McPherson, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'David Fierro, Atau Tanaka' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 ACM Designing Interactive Systems Conference</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_DIS24_SonicEntanglements.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper investigates sound and music interactions arising from the use of electromyography (EMG) to instrumentalise signals from muscle exertion of the human body. We situate EMG within a family of embodied interaction modalities, where it occupies a middle ground, considered as a “signal from the inside” compared with external observations of the body (e.g., motion capture), but also seen as more volitional than neurological states recorded by brain electroencephalogram (EEG). To understand the messiness of gestural interaction afforded by EMG, we revisit the phenomenological turn in HCI, reading Paul Dourish’s work on the transparency of “ready-to-hand” technologies against the grain of recent posthumanist theories, which offer a performative interpretation of musical entanglements between bodies, signals, and representations. We take music performance as a use case, reporting on the opportunities and constraints posed by EMG in workshop-based studies of vocal, instrumental, and electronic practices. We observe that across our diverse range of musical subjects, they consistently challenged notions of EMG as a transparent tool that directly registered the state of the body, reporting instead that it took on “present-at-hand” qualities, defamiliarising the performer’s own sense of themselves and reconfiguring their embodied practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_DIS24_SonicEntanglements</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sonic Entanglements with Electromyography: Between Bodies, Signals, and Representations}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Morrison, Landon and McPherson, Andrew P. and Fierro, David and Tanaka, Atau}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2024 ACM Designing Interactive Systems Conference}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Copenhagen, Denmark}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{DIS '24}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2691–2707}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3643834.3661572}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400705830}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3643834.3661572}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Sabnis_EH24_MotionVibration.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Sabnis_EH24_MotionVibration.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sabnis_EH24_MotionVibration" class="col-sm-7"> <div class="title">Design and Experience Tactile Symbols using Continuous and Motion-Coupled Vibration</div> <div class="author"> Nihar Sabnis, Dennis Wittchen, Gabriela Vega, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Courtney N. Reed, Paul Strohmeier' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Hands-on Demonstrations at the Eurohaptics Conference</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Demo Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Sabnis_EH24_MotionVibration.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>One application area of vibrotactile haptics has been to create abstract tactile symbols, such as notifications, which requires user’s interpretation. The other area is the rendering of realistic material interactions, such as friction, which provide embodied experiences to the users. The abstract symbols are ren- dered using continuous vibration, whereas vibration coupled to user motion is used to elicit embodied experiences. In our research, we explore how embodied experiences can be used for symbolic mediation, i.e.: how can we use these two vibration types to design hybrid tactile symbols? In this demo, we invite visitors to explore a set of such hybrid tactile symbols created by experts in a user study [1]. We further invite visitors to design tactile symbols themselves using a graphical user interface and experience them on a set of tangible user interfaces. We expect the demo to take 3–5 minutes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sabnis_EH24_MotionVibration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Design and Experience Tactile Symbols using Continuous and Motion-Coupled Vibration}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sabnis, Nihar and Wittchen, Dennis and Vega, Gabriela and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Hands-on Demonstrations at the Eurohaptics Conference}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Lille, France}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_MSX_AAFVocalAccuracy.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_MSX_AAFVocalAccuracy.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_MSX_AAFVocalAccuracy" class="col-sm-7"> <div class="title">Auditory imagery ability influences accuracy when singing with altered auditory feedback</div> <div class="author"> <em>Courtney N. Reed</em>, Marcus Pearce, and Andrew McPherson</div> <div class="periodical"> <em>Musicae Scientiae</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Journal Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_MSX_AAFVocalAccuracy.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this preliminary study, we explored the relationship between auditory imagery ability and the maintenance of tonal and temporal accuracy when singing and audiating with altered auditory feedback (AAF). Actively performing participants sang and audiated (sang mentally but not aloud) a self-selected piece in AAF conditions, including upward pitch-shifts and delayed auditory feedback (DAF), and with speech distraction. Participants with higher self-reported scores on the Bucknell Auditory Imagery Scale (BAIS) produced a tonal reference that was less disrupted by pitch shifts and speech distraction than musicians with lower scores. However, there was no observed effect of BAIS score on temporal deviation when singing with DAF. Auditory imagery ability was not related to the experience of having studied music theory formally, but was significantly related to the experience of performing. The significant effect of auditory imagery ability on tonal reference deviation remained even after partialling out the effect of experience of performing. The results indicate that auditory imagery ability plays a key role in maintaining an internal tonal center during singing but has at most a weak effect on temporal consistency. In this article, we outline future directions in understanding the multifaceted role of auditory imagery ability in singers’ accuracy and expression.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Reed_MSX_AAFVocalAccuracy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Auditory imagery ability influences accuracy when singing with altered auditory feedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Pearce, Marcus and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Musicae Scientiae}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SAGE Publications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{478–501}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/10298649231223077}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2045-4147}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1177/10298649231223077}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Rallabandi_TEI24_BaseStitch.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Rallabandi_TEI24_BaseStitch.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Rallabandi_TEI24_BaseStitch" class="col-sm-7"> <div class="title">Base and Stitch: Evaluating eTextile Interfaces from a Material-Centric View</div> <div class="author"> L. Vineetha Rallabandi, Alice C. Haynes, <em>Courtney N. Reed</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Paul Strohmeier' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Rallabandi_TEI24_BaseStitch.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Fabrics are seen as the foundation for e-textile interfaces but contribute their own tactile properties to interaction. We examine the role of fabrics in gestural interaction from a novel, textile-focused view. We replicated an eTextile sensor and interface for rolling and pinching gestures on four different fabric swatches and invited 6 participants, including both designers and lay-users, to interact with them. Using a semi-structured interview, we examined their interaction with the materials and how they perceived movement and feedback from the textile sensor and a visual GUI. We analyzed participants’ responses using a joint, reflexive thematic analysis and propose two key considerations for research in e-textile design: 1) Both sensor and fabric contribute their own, inseparable materiality and 2) Wearable sensing must be evaluated with respect to culturally situated bodies and orientation. Expanding on material-oriented design research, we proffer that the evaluation of eTextiles must also be material-led and cannot be decontextualized and must be grounded within a soma-aware and situated context.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Rallabandi_TEI24_BaseStitch</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Base and Stitch: Evaluating eTextile Interfaces from a Material-Centric View}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rallabandi, L. Vineetha and Haynes, Alice C. and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Cork, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '24}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3623509.3633363}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400704024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3623509.3633363}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Freire_TEI24_RaveNETPerf.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Freire_TEI24_RaveNETPerf.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Freire_TEI24_RaveNETPerf" class="col-sm-7"> <div class="title">Liminal Space: A Performance with RaveNET</div> <div class="author"> Rachel Freire, Valentin Martinez-Missir, <em>Courtney N. Reed</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Paul Strohmeier' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Art/Performance Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Freire_TEI24_RaveNETPerf.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present our musical performance exploration of liminal spaces, which focuses on the interconnected physicality of bodies in music, using biosignals and gestural, movement-based interaction to shape live performances in novel ways. Physical movement is important in structuring performance, providing cues across musical ensembles, and non-verbally informing other musicians of intention. This is especially true for improvised work. Our performance involves the use of our musicking bodies to modulate audio signals. Three bespoke wearable nodes modulate the performance through control voltages (CV) and interface with specific technical aspects of our instruments and techniques: 1) an “anti-corset” that measures the expansion and resistance of Reed’s abdomen while singing, 2) an augmented glove that assists Strohmeier’s bass/guitar signal routing across his pedal board and modular setup, and 3) a cap-like device that captures Martinez-Missir’s subtle facial expressions as he manipulates his modular synthesizer and drum machine setup. Through these performances we explore the notion of control in musical improvised performance, the interconnectedness and communications between our ensemble as we learn to collaborate and interpret each others’ bodies in this novel interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Freire_TEI24_RaveNETPerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Liminal Space: A Performance with RaveNET}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Freire, Rachel and Martinez-Missir, Valentin and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Cork, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '24}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{105}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3623509.3635337}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400704024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3623509.3635337}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Freire_TEI24_RaveNETWiP.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Freire_TEI24_RaveNETWiP.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Freire_TEI24_RaveNETWiP" class="col-sm-7"> <div class="title">RaveNET: Connecting People and Exploring Liminal Space through Wearable Networks in Music Performance</div> <div class="author"> Rachel Freire, Valentin Martinez-Missir, <em>Courtney N. Reed</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Paul Strohmeier' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Freire_TEI24_RaveNETWiP.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>RaveNET connects people to music, enabling musicians to modulate sound using signals produced by their own bodies or the bodies of others. We present three wearable prototype nodes in an inaugural RaveNET performance: Bones, an anti-corset, uses capacitive sensing to detect stretch as the singer breathes. Tendons, a half-glove, measures galvanic skin response, pulse, and movement of the bass player’s hands. Veins, a cap with electrodes for surface electromyography, captures the facial expressions of the drum machine operator. These signals are filtered, normalized, and amplified to control voltage levels to modulate sound. Together, musicians and nodes form RaveNET and engage with shared liminal experiences. In designing these wearables and evaluating them in performance, we reflect on our creative processes, spaces between our different bodies, our presence and control within the network, and how this made us adapt our movements in order to be noticed and heard.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Freire_TEI24_RaveNETWiP</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{RaveNET: Connecting People and Exploring Liminal Space through Wearable Networks in Music Performance}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Freire, Rachel and Martinez-Missir, Valentin and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Cork, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '24}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{89}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3623509.3635270}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400704024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3623509.3635270}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/BryanKinns_CreatingDigitally.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="BryanKinns_CreatingDigitally.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BryanKinns_CreatingDigitally" class="col-sm-7"> <div class="title">A Guide to Evaluating the Experience of Media and Arts Technology</div> <div class="author"> Nick Bryan-Kinns, and <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Creating Digitally. Intelligent Systems Reference Library</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>Book Chapter</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/BryanKinns_CreatingDigitally.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Evaluation is essential to understanding the value that digital creativity brings to people’s experience, for example in terms of their enjoyment, creativity, and engagement. There is a substantial body of research on how to design and evaluate interactive arts and digital creativity applications. There is also extensive Human-Computer Interaction (HCI) literature on how to evaluate user interfaces and user experiences. However, it can be difficult for artists, practitioners, and researchers to navigate such a broad and disparate collection of materials when considering how to evaluate technology they create that is at the intersection of art and interaction. This chapter provides a guide to designing robust user studies of creative applications at the intersection of art, technology and interaction, which we refer to as Media and Arts Technology (MAT). We break MAT studies down into two main kinds: proof-of-concept and comparative studies. As MAT studies are exploratory in nature, their evaluation requires the collection and analysis of both qualitative data such as free text questionnaire responses, interviews, and observations, and also quantitative data such as questionnaires, number of interactions, and length of time spent interacting. This chapter draws on over 20 years of experience of designing and evaluating novel interactive systems to provide a concrete template on how to structure a study to evaluate MATs that is both rigorous and repeatable, and how to report study results that are publishable and accessible to a wide readership in art and science communities alike.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">BryanKinns_CreatingDigitally</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{A Guide to Evaluating the Experience of Media and Arts Technology}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bryan-Kinns, Nick and Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Creating Digitally. Intelligent Systems Reference Library}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{267–300}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-031-31360-8_10}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9783031313608}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1868-4408}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1007/978-3-031-31360-8_10}</span><span class="p">,</span>
  <span class="na">vol</span> <span class="p">=</span> <span class="s">{241}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Yang_Computer_Hear.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Yang_Computer_Hear.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yang_Computer_Hear" class="col-sm-7"> <div class="title">Do You Hear What I Hear?</div> <div class="author"> Simin Yang, Mathieu Barthet, <em>Courtney N. Reed</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Elaine Chew' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Computer</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Journal Article</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Spotlight on Transactions</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Yang_Computer_Hear.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This installment of Computer’s series highlighting the work published in IEEE Computer Society journals comes from IEEE Transactions on Affective Computing. Musical performance is often described as expressing emotion. However, the human perception of emotion in music is not well understood. The studies by Yang et al. examine listeners’ emotional perception over time to a performance of a single musical piece experienced in live concert conditions, and in the lab, through video recordings. The authors aimed to find out the following: What level of agreement exists between listeners of the same performance? How are perceived emotions related to the semantic features of the music (expressible in linguistic terms) and to machine-extractable music features? What aspects of the music itself and of the listener, like music expertise, influence perceived emotions?</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Yang_Computer_Hear</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Do You Hear What I Hear?}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Simin and Barthet, Mathieu and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Institute of Electrical and Electronics Engineers (IEEE)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{56}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4–6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/mc.2023.3315470}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1558-0814}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1109/MC.2023.3315470}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Solinski_CinC23_TriangleSimplex.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Solinski_CinC23_TriangleSimplex.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Solinski_CinC23_TriangleSimplex" class="col-sm-7"> <div class="title">Triangle Simplex Plots for Representing and Classifying Heart Rate Variability</div> <div class="author"> Mateusz Soliński, <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In Proceedings of the 2023 Computing in Cardiology Conference (CinC)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Best Poster</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Solinski_CinC23_TriangleSimplex.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Simplex plots afford barycentric mapping and visualisation of the ratio of three variables, summed to a constant, as positions in an equilateral triangle (2-simplex); for instance, time distribution in three-interval musical rhythms. We propose a novel use of simplex plots to visualise the balance of autonomic variables and classification of autonomic states during baseline and music performance. RR interval series extracted from electrocardiographic (ECG) traces were collected from a musical trio (pianist, violinist, cellist) in a baseline (5 min) and music performance (\sim10 min) condition. Schubert’s Trio Op. 100, \textitAndante con moto was performed in nine rehearsal sessions over five days. Each RR interval series’ very low (VLF), low (LF), and high (HF) frequency component power values, calculated in 30 sec windows (hop size 15 sec), were normalised to 1 and visualised in triangle simplex plots. Spectral clustering was used to cluster data points for baseline and music conditions. We correlated the accuracy between the clustered and true values. Strong negative correlation was observed for the violinist (r = –0.80, p ≤.01, accuracy range: [0.64, 0.94]) and pianist (r = –0.62, p = .073, [0.64, 0.80]), suggesting adaptation of their cardiac response (reduction between baseline and performance) over the performances; a weakly negative, non-significant correlation was observed for the cellist (r = –0.23, p = .545, [0.50, 0.61]), indicating similarity between baseline and performance over time. Using simplex plots, we were able to effectively represent VLF, LF and HF ratios and track changes in autonomic response over a series of music rehearsals to observe autonomic states and changes over time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Solinski_CinC23_TriangleSimplex</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Triangle Simplex Plots for Representing and Classifying Heart Rate Variability}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliński, Mateusz and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 Computing in Cardiology Conference (CinC)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Solinski_CinC23_TRDanalysis.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Solinski_CinC23_TRDanalysis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Solinski_CinC23_TRDanalysis" class="col-sm-7"> <div class="title">Time Delay Stability Analysis of Pairwise Interactions Amongst Ensemble-Listener RR Intervals and Expressive Music Features</div> <div class="author"> Mateusz Soliński, <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In Proceedings of 2023 Computing in Cardiology Conference (CinC)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Best Poster</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Solinski_CinC23_TRDanalysis.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Time Delay Stability (TDS) can reveal physiological function and states in networked organs. Here, we introduce a novel application of TDS to a musical setting to study interactions between RR intervals of ensemble musicians and a listener, and music properties. Three musicians performed a movement from Schubert’s Trio Op. 100 nine times in the company of one listener. Their RR intervals were collected during baseline (5 min, silence) and performances (\sim10 min each). Loudness and tempo were extracted from recorded music audio. Regions of stable optimal time delay were identified during baseline and music, shuffled data, and data pairs from incongruent recordings. Bootstrapping was employed to obtain mean TDS probabilities (calculated based on all performances). A significant difference in mean TDS probability between music and baseline is observed for all musician pairs (p&lt;.001) and for cello-listener (p=.025); mean TDS probability being greater during music. A significant decrease in mean TDS probability was observed for piano-violin (p&lt;.001), violin-tempo (p=.045), and cello-tempo (p&lt;.001) for incongruent pairs. The highest inter-musician TDS probabilities were observed in musically tense sections: the final climax before the music dies down for the ending and mid piece in a suspenseful swell. This framework offers a promising way to track dynamic RR interval interactions between people engaged in a shared activity, and, in this musical activity, between the people and music properties.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Solinski_CinC23_TRDanalysis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Time Delay Stability Analysis of Pairwise Interactions Amongst Ensemble-Listener RR Intervals and Expressive Music Features}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soliński, Mateusz and Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of 2023 Computing in Cardiology Conference (CinC)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Armitage_AIMC23_AgentialInstruments.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Armitage_AIMC23_AgentialInstruments.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Armitage_AIMC23_AgentialInstruments" class="col-sm-7"> <div class="title">Agential Instruments Design Workshop</div> <div class="author"> Jack Armitage, Victor Shepardson, Nicola Privato, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Teresa Pelinski, Adan L. Benito Temprano, Lewis Wolstanholme, Andrea Martelloni, Franco Santiago Caspe, Courtney N. Reed, Sophie Skach, Rodrigo Diaz, Sean Patrick O’Brien, Jordie Shier' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on AI and Music Creativity</em>, Aug 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Armitage_AIMC23_AgentialInstruments.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Physical and gestural musical instruments that take advantage of artificial intelligence and machine learning to explore instrumental agency are becoming more accessible due to the development of new tools and workflows specialised for mobility, portability, efficiency and low latency. This full-day, hands-on workshop will provide all of these tools to participants along with support from their creators, enabling rapid creative exploration of their applications a musical instrument design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Armitage_AIMC23_AgentialInstruments</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Agential Instruments Design Workshop}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Armitage, Jack and Shepardson, Victor and Privato, Nicola and Pelinski, Teresa and Temprano, Adan L. Benito and Wolstanholme, Lewis and Martelloni, Andrea and Caspe, Franco Santiago and Reed, Courtney N. and Skach, Sophie and Diaz, Rodrigo and O'Brien, Sean Patrick and Shier, Jordie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on AI and Music Creativity}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Brighton, UK}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_NIME23_QueryingExperience.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME23_QueryingExperience.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME23_QueryingExperience" class="col-sm-7"> <div class="title">Querying Experience with Musical Interaction</div> <div class="author"> <em>Courtney N. Reed</em>, Eevee Zayas-Garin, and Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, May 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME23_QueryingExperience.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With this workshop, we aim to bring together researchers with the common interest of querying, articulating and understanding experience in the context of New Interfaces for Musical Expression, and to jointly identify challenges, methodologies and opportunities in this space. Furthermore, we hope it serves as a platform for strengthening the community of researchers working with qualitative and phenomenological methods around the design of DMIs and HCI applied to musical interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME23_QueryingExperience</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Querying Experience with Musical Interaction}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Zayas-Garin, Eevee and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Mexico City, Mexico}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_VocalMetaphor.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_VocalMetaphor" class="col-sm-7"> <div class="title">Negotiating Experience and Communicating Information Through Abstract Metaphor</div> <div class="author"> <em>Courtney N. Reed</em>, Paul Strohmeier, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_VocalMetaphor.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>An implicit assumption in metaphor use is that it requires grounding in a familiar concept, prominently seen in the popular Desktop Metaphor. In human-to-human communication, however, abstract metaphors, without such grounding, are often used with great success. To understand when and why metaphors work, we present a case study of metaphor use in voice teaching. Voice educators must teach about subjective, sensory experiences and rely on abstract metaphor to express information about unseen and intangible processes inside the body. We present a thematic analysis of metaphor use by 12 voice teachers. We found that metaphor works not because of strong grounding in the familiar, but because of its ambiguity and flexibility, allowing shared understanding between individual lived experiences. We summarise our findings in a model of metaphor-based communication. This model can be used as an analysis tool within the existing taxonomies of metaphor in user interaction for better understanding why metaphor works in HCI. It can also be used as a design resource for thinking about metaphor use and abstracting metaphor strategies from both novel and existing designs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_VocalMetaphor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Negotiating Experience and Communicating Information Through Abstract Metaphor}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580700}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580700}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Sabnis_CHI23_TactileSymbols.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Sabnis_CHI23_TactileSymbols.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sabnis_CHI23_TactileSymbols" class="col-sm-7"> <div class="title">Tactile Symbols with Continuous and Motion-Coupled Vibration: An Exploration of Using Embodied Experiences for Hermeneutic Design</div> <div class="author"> Nihar Sabnis, Dennis Wittchen, Gabriela Vega, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Courtney N. Reed, Paul Strohmeier' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Sabnis_CHI23_TactileSymbols.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With most digital devices, vibrotactile feedback consists of rhythmic patterns of continuous vibration. In contrast, when interacting with physical objects, we experience many of their material properties through vibration which is not continuous, but dynamically coupled to our actions. We assume the first style of vibration to lead to hermeneutic mediation, while the second style leads to embodied mediation. What if both types of mediation could be used to design tactile symbols? To investigate this, five haptic experts designed tactile symbols using continuous and motion-coupled vibration. Experts were interviewed to understand their symbols and design approach. A thematic analysis revealed themes showing that lived experience and affective qualities shaped design choices, that experts optimized for passive or active symbols, and that they considered context as part of the design. Our study suggests that adding embodied experiences as a design resource changes how participants think of tactile symbol design, thus broadening the scope of the symbol by design for context, and expanding their affective repertoire as changing the type of vibration influences perceived valence and arousal.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sabnis_CHI23_TactileSymbols</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Tactile Symbols with Continuous and Motion-Coupled Vibration: An Exploration of Using Embodied Experiences for Hermeneutic Design}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sabnis, Nihar and Wittchen, Dennis and Vega, Gabriela and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{688}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3581356}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3581356}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Sabnis_CHI23_HapticServos.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Sabnis_CHI23_HapticServos.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sabnis_CHI23_HapticServos" class="col-sm-7"> <div class="title">Haptic Servos: Self-Contained Vibrotactile Rendering System for Creating or Augmenting Material Experiences</div> <div class="author"> <em>Courtney N. Reed*</em>, Nihar Sabnis*, Dennis Wittchen*, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Narjes Pourjafarian, Jürgen Steimle, Paul Strohmeier' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Honorable Mention</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Sabnis_CHI23_HapticServos.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>When vibrations are synchronized with our actions, we experience them as material properties. This has been used to create virtual experiences like friction, counter-force, compliance, or torsion. Implementing such experiences is non-trivial, requiring high temporal resolution in sensing, high fidelity tactile output, and low latency. To make this style of haptic feedback more accessible to non-domain experts, we present Haptic Servos: self-contained haptic rendering devices which encapsulate all timing-critical elements. We characterize Haptic Servos’ real-time performance, showing the system latency is &lt;5 ms. We explore the subjective experiences they can evoke, highlighting that qualitatively distinct experiences can be created based on input mapping, even if stimulation parameters and algorithm remain unchanged. A workshop demonstrated that users new to Haptic Servos require approximately ten minutes to set up a basic haptic rendering system. Haptic Servos are open source, we invite others to copy and modify our design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Sabnis_CHI23_HapticServos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Haptic Servos: Self-Contained Vibrotactile Rendering System for Creating or Augmenting Material Experiences}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed*, Courtney N. and Sabnis*, Nihar and Wittchen*, Dennis and Pourjafarian, Narjes and Steimle, J\"{u}rgen and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{522}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580716}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580716}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_CHI23_BodyLutherie.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_BodyLutherie.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_BodyLutherie" class="col-sm-7"> <div class="title">As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Body X Materials</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Position Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_BodyLutherie.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through soma-centric research, we see the different interaction roles of our bodies: they are the locus of our experience, a conduit for our expression and engagement, a sensor of feedback in the world, and a collaborator in our interaction with it. More" traditional" examinations of the body might look at control over it; for instance, in my research around vocal embodiment, I see many teachers and practitioners alike talking about how we can maintain control over the body. However, bodies are living, inconsistent, and typically weird. In reality, we do not have as much control over them as we would like or think we do. In this position paper, I will touch on my research around vocal physiology and sonified and vibrotactile feedback as I frame our role in a new light—designers as Body Luthiers, who must address the body as a material with inconsistencies, flaws, and variability, and work with it as a partner, embracing its uniqueness and changeability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_BodyLutherie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Body X Materials}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Wittchen_AHs23_AugmentedShoes.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Wittchen_AHs23_AugmentedShoes.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wittchen_AHs23_AugmentedShoes" class="col-sm-7"> <div class="title">Designing Interactive Shoes for Tactile Augmented Reality</div> <div class="author"> Dennis Wittchen, Valentin Martinez-Missir, Sina Mavali, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Nihar Sabnis, Courtney N. Reed, Paul Strohmeier' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2023</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Wittchen_AHs23_AugmentedShoes.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Augmented Footwear has become an increasingly common research area. However, as this is a comparatively new direction in HCI, researchers and designers are not able to build upon common platforms. We discuss the design space of shoes for augmented tactile reality, focussing on physiological and biomechanical factors as well as technical considerations. We present an open source example implementation from this space, intended as an experimental platform for vibrotactile rendering and tactile AR and provide details on experiences that could be evoked with such a system. Anecdotally, the new prototype provided experiences of material properties like compliance, as well as altered perception of their movements and agency. We intend our work to lower the barrier of entry for new researchers and to support the field of tactile rendering in footwear in general by making it easier to compare results between studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Wittchen_AHs23_AugmentedShoes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Designing Interactive Shoes for Tactile Augmented Reality}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wittchen, Dennis and Martinez-Missir, Valentin and Mavali, Sina and Sabnis, Nihar and Reed, Courtney N. and Strohmeier, Paul}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2023}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Glasgow, United Kingdom}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '23}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–14}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3582700.3582728}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399845}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3582700.3582728}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_TEI23_BodyAsSound.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI23_BodyAsSound.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI23_BodyAsSound" class="col-sm-7"> <div class="title">The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI23_BodyAsSound.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Multi-sensory experiences underpin embodiment, whether with the body itself or technological extensions of it. Vocalists experience intensely personal embodiment, as vocalisation has few outwardly visible effects and kinaesthetic sensations occur largely within the body, rather than through external touch. We explored this embodiment using a probe which sonified laryngeal muscular movements and provided novel auditory feedback to two vocalists over a month-long period. Somatic and micro-phenomenological approaches revealed that the vocalists understand their physiology through its sound, rather than awareness of the muscular actions themselves. The feedback shaped the vocalists’ perceptions of their practice and revealed a desire for reassurance about exploration of one’s body when the body-as-sound understanding was disrupted. Vocalists experienced uncertainty and doubt without affirmation of perceived correctness. This research also suggests that technology is viewed as infallible and highlights expectations that exist about its ability to dictate success, even when we desire or intend to explore.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI23_BodyAsSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Warsaw, Poland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3569009.3572738}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399777}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3569009.3572738}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Haynes_TEI23_BeingMeaningful.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Haynes_TEI23_BeingMeaningful.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Haynes_TEI23_BeingMeaningful" class="col-sm-7"> <div class="title">Being Meaningful: Weaving Soma-Reflective Technological Mediations into the Fabric of Daily Life</div> <div class="author"> Alice Haynes, <em>Courtney N. Reed</em>, Charlotte Nordmoen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sophie Skach' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-light p-2" disabled>Workshop Proposal</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Haynes_TEI23_BeingMeaningful.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A one-size-fits-all design mentality, rooted in objective efficiency, is ubiquitous in our mass-production society. This can negate peoples’ experiences, bodies, and narratives. Ongoing HCI research proposes design for meaningful relations; but for many researchers, the practical implementation of these philosophies remains somewhat intangible. In this Studio, we playfully tackle this space by engaging with the nuances of soft, flexible, and organic materials, collectively designing probes to embrace plurality, embody meaning, and encourage soma-reflection. Focusing on materiality and practices from e-textiles, soft robotics, and biomaterials research, we address technology’s role as a mediator of our experiences and determiner of our realities. The processes and probes developed in this Studio will serve as an experiential manifesto, providing practitioners with tools to deepen their own practices for designing soma-reflective tangible and embodied interaction. The Studio will form the first steps for ongoing collaboration, focusing on bespoke design and curation of meaningful, personal relationships.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Haynes_TEI23_BeingMeaningful</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Being Meaningful: Weaving Soma-Reflective Technological Mediations into the Fabric of Daily Life}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Haynes, Alice and Reed, Courtney N. and Nordmoen, Charlotte and Skach, Sophie}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Warsaw, Poland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3569009.3571844}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399777}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3569009.3571844}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_PhD_ImaginingSensing.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_PhD_ImaginingSensing.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_PhD_ImaginingSensing" class="col-sm-7"> <div class="title">Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>PhD Computer Science, Queen Mary University of London</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>PhD Thesis</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>ACM SIGCHI Outstanding Dissertation</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_PhD_ImaginingSensing.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The voice is body and instrument. Third-person interpretation of the voice by listeners, vocal teachers, and digital agents is centred largely around audio feedback. For a vocalist, physical feedback from within the body provides an additional interaction. The vocalist’s understanding of their multi-sensory experiences is through tacit knowledge of the body. This knowledge is difficult to articulate, yet awareness and control of the body are innate. In the ever-increasing emergence of technology which quantifies or interprets physiological processes, we must remain conscious also of embodiment and human perception of these processes. Focusing on the vocalist-voice relationship, this thesis expands knowledge of human interaction and how technology influences our perception of our bodies. To unite these different perspectives in the vocal context, I draw on mixed methods from cognitive science, psychology, music information retrieval, and interactive system design. Objective methods such as vocal audio analysis provide a third-person observation. Subjective practices such as micro-phenomenology capture the experiential, first-person perspectives of the vocalists themselves. Quantitative-qualitative blend provides details not only on novel interaction, but also an understanding of how technology influences existing understanding of the body.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Reed_PhD_ImaginingSensing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{PhD Computer Science, Queen Mary University of London}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_NIME22_Microphenomenology.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME22_Microphenomenology.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME22_Microphenomenology" class="col-sm-7"> <div class="title">Exploring Experiences with New Musical Instruments through Micro-phenomenology</div> <div class="author"> <em>Courtney N. Reed</em>, Charlotte Nordmoen, Andrea Martelloni, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Giacomo Lepri, Nicole Robson, Eevee Zayas-Garin, Kelsey Cotton, Lia Mice, Andrew McPherson' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME22_Microphenomenology.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five microphenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME22_Microphenomenology</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Exploring Experiences with New Musical Instruments through Micro-phenomenology}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{The University of Auckland, New Zealand}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{49}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21428/92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.21428%2F92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/-Ket6l90S8I}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_CommunicatingBodies.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_CommunicatingBodies" class="col-sm-7"> <div class="title">Communicating Across Bodies in the Voice Lesson</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Tangible Interaction for Well-Being</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Position Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_CommunicatingBodies.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this position paper, I would like to introduce my research on vocalists and their relationships with their bodies, and how the use of haptic feedback can improve these connections and the way we communicate sensory experience. I use the voice lesson and vocal performance as an environment to understand more broadly how people perceive very refined movements which they feel internally. My research seeks to understand how we communicate these sensory experiences in human-to-human interaction and how we can augment or communicate sensory experience through technology. I examine perception of these experiences through different feedback modalities, namely auditory and haptic feedback. Providing new ways to communicate our sensory experiences can lead to improvements in understanding between two individuals (for instance teacher and student). In virtual singing lessons, where the majority of voice study is being done in early 2022, this is especially important, as many of the common ways of interacting with the voice have disappeared with the transition to online interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_CommunicatingBodies</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Communicating Across Bodies in the Voice Lesson}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Tangible Interaction for Well-Being}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_CHI22_SensorySketching.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_SensorySketching.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_SensorySketching" class="col-sm-7"> <div class="title">Sensory Sketching for Singers</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Sketching Across the Senses</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Position Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_SensorySketching.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This position paper outlines my study of vocalists and the relationships with the voice as both instrument and part of the body. I study this embodiment through a phenomenological perspective, employing somaesthetics and micro-phenomenology to explore the tacit relationships that singers have with their body. While verbal metaphor is traditionally used to articulate experience in teaching voice, I also use body mapping and material speculation to help articulate tactile and auditory experiences while singing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_SensorySketching</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sensory Sketching for Singers}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Sketching Across the Senses}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_AHs22_SingingKnit.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_AHs22_SingingKnit.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_AHs22_SingingKnit" class="col-sm-7"> <div class="title">Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances</div> <div class="author"> <em>Courtney N. Reed</em>, Sophie Skach, Paul Strohmeier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew P. McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2022</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_AHs22_SingingKnit.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer’s vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_AHs22_SingingKnit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Skach, Sophie and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2022}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kashiwa, Chiba, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{170–183}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3519391.3519412}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450396325}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3519391.3519412}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_TEI22_Vibrotouch.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI22_Vibrotouch.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI22_Vibrotouch" class="col-sm-7"> <div class="title">Vibro-Touch</div> <div class="author"> <em>Courtney N. Reed</em>, and Nihar Sabnis</div> <div class="periodical"> <em>In ACM TEI Studio “How Tangible is TEI?” Exploring Swatches as a New Academic Publication Format</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Position Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI22_Vibrotouch.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In our research, we examine tactile representations which are used for user interaction and system notifications. This swatch works as an interface to store and playback vibrotactile stimuli. This allows for easy, cost effective (\~€20) reproduction and exploration of tactile feedback. Typically, feedback designed and used in research is described through written formats. This swatch provides a companion to physically experience the vibrations. The tactile sensation is stored on a microcontroller and played back through a speaker which works as an actuator. The swatch could be given to others to test and experience different sensations in a simplified, modular format. For instance, rather than redesigning feedback for each new study, the tactile feedback could be shared and reproduced for new research. The code on the microcontroller can be changed or updated to have multiple "swatches" in one. In other applications, the swatch could also play audio feedback.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI22_Vibrotouch</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Vibro-Touch}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Sabnis, Nihar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM TEI Studio ``How Tangible is TEI?'' Exploring Swatches as a New Academic Publication Format}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daejeon, Republic of Korea}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI22_EmbodiedSingingDC.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI22_EmbodiedSingingDC" class="col-sm-7"> <div class="title">Examining Embodied Sensation and Perception in Singing</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI22_EmbodiedSingingDC.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces my PhD research on the relationship which vocalists have with their voice. The voice, both instrument and body, provides a unique perspective to examine embodied practice. The interaction with the voice is largely without a physical interface and it is difficult to describe the sensation of singing; however, voice pedagogy has been successful at using metaphor to communicate sensory experience between student and teacher. I examine the voice through several different perspectives, including experiential, physiological, and communicative interactions, and explore how we convey sensations in voice pedagogy and how perception of the body is shaped through experience living in it. Further, through externalising internal movement using sonified surface electromyography, I aim to give presence to aspects of vocal movement which have become subconscious or automatic. The findings of this PhD will provide understanding of how we perceive the experience of living within the body and perform through using the body as an instrument.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI22_EmbodiedSingingDC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining Embodied Sensation and Perception in Singing}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daejeon, Republic of Korea}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '22}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3490149.3503581}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450391474}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3490149.3503581}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/BryanKinns_NeurIPS_XAImusic.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="BryanKinns_NeurIPS_XAImusic.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BryanKinns_NeurIPS_XAImusic" class="col-sm-7"> <div class="title">Exploring XAI for the Arts: Explaining Latent Space in Generative Music</div> <div class="author"> Nick Bryan-Kinns, Berker Banar, Corey Ford, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Courtney N. Reed, Yixiao Zhang, Simon Colton, Jack Armitage' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In 1st Workshop on eXplainable AI Approaches for Debugging and Diagnosis (XAI4Debugging@NeurIPS2021)</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Position Paper</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/BryanKinns_NeurIPS_XAImusic.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Explainable AI has the potential to support more interactive and fluid co-creative AI systems which can creatively collaborate with people. To do this, creative AI models need to be amenable to debugging by offering eXplainable AI (XAI) features which are inspectable, understandable, and modifiable. However, currently there is very little XAI for the arts. In this work, we demonstrate how a latent variable model for music generation can be made more explainable; specifically we extend MeasureVAE which generates measures of music. We increase the explainability of the model by: i) using latent space regularisation to force some specific dimensions of the latent space to map to meaningful musical attributes, ii) providing a user interface feedback loop to allow people to adjust dimensions of the latent space and observe the results of these changes in real-time, iii) providing a visualisation of the musical attributes in the latent space to help people understand and predict the effect of changes to latent space dimensions. We suggest that in doing so we bridge the gap between the latent space and the generated musical outcomes in a meaningful way which makes the model and its outputs more explainable and more debuggable.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">BryanKinns_NeurIPS_XAImusic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Exploring XAI for the Arts: Explaining Latent Space in Generative Music}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bryan-Kinns, Nick and Banar, Berker and Ford, Corey and Reed, Courtney N. and Zhang, Yixiao and Colton, Simon and Armitage, Jack}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{1st Workshop on eXplainable AI Approaches for Debugging and Diagnosis (XAI4Debugging@NeurIPS2021)}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2308.05496}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_ICMPC21_AAF.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_ICMPC21_AAF.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_ICMPC21_AAF" class="col-sm-7"> <div class="title">The Role of Auditory Imagery and Altered Auditory Feedback in Singers’ Timing Accuracy</div> <div class="author"> <em>Courtney N. Reed</em>, Andrew P. McPherson, and Marcus T. Pearce</div> <div class="periodical"> <em>In Proceedings of the Joint 16th International Conference on Music Perception and Cognition (ICMPC) and 11th Triennial Conference of the European Society for the Cognitive Science of Music (ESCOM)</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>Best Paper Nomination</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_ICMPC21_AAF.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Auditory imagery allows musicians to recall mental representations of sound and has been linked to better sensorimotor coordination, effective gestural communication with other performers, and the ability to perform with timing accuracy even when auditory feedback is disrupted. The predominance of auditory imagery in the multimodal relationships driving internal temporal models however remains unclear. This study explores how singers adapt to altered auditory feedback (AAF) using auditory imagery. We examine whether auditory imagery ability, measured using the Bucknell Auditory Imagery Scale, affects singers’ ability to maintain temporal accuracy when singing and audiating with AAF and explore the significance of auditory imagery on timing and its role in multimodal imagery. Additionally, we focus on how imagery benefits musicians specifically, comparing timing error in a group of skilled performers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_ICMPC21_AAF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The Role of Auditory Imagery and Altered Auditory Feedback in Singers' Timing Accuracy}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P. and Pearce, Marcus T.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Joint 16th International Conference on Music Perception and Cognition (ICMPC) and 11th Triennial Conference of the European Society for the Cognitive Science of Music (ESCOM)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Sheffield, UK}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Yang_TAFFC_emotionperception.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Yang_TAFFC_emotionperception.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yang_TAFFC_emotionperception" class="col-sm-7"> <div class="title">Examining Emotion Perception Agreement in Live Music Performance</div> <div class="author"> Simin Yang, <em>Courtney N. Reed</em>, Elaine Chew, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Mathieu Barthet' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Journal Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Yang_TAFFC_emotionperception.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Current music emotion recognition (MER) systems rely on emotion data averaged across listeners and over time to infer the emotion expressed by a musical piece, often neglecting time- and listener-dependent factors. These limitations can restrict the efficacy of MER systems and cause misjudgements. We present two exploratory studies on music emotion perception. First, in a live music concert setting, fifteen audience members annotated perceived emotion in the valence-arousal space over time using a mobile application. Analyses of inter-rater reliability yielded widely varying levels of agreement in the perceived emotions. A follow-up lab-based study to uncover the reasons for such variability was conducted, where twenty-one participants annotated their perceived emotions whilst viewing and listening to a video recording of the original performance and offered open-ended explanations. Thematic analysis revealed salient features and interpretations that help describe the cognitive processes underlying music emotion perception. Some of the results confirm known findings of music perception and MER studies. Novel findings highlight the importance of less frequently discussed musical attributes, such as musical structure, performer expression, and stage setting, as perceived across audio and visual modalities. Musicians are found to attribute emotion change to musical harmony, structure, and performance technique more than non-musicians. We suggest that accounting for such listener-informed music features can benefit MER in helping to address variability in emotion perception by providing reasons for listener similarities and idiosyncrasies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Yang_TAFFC_emotionperception</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining Emotion Perception Agreement in Live Music Performance}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Simin and Reed, Courtney N. and Chew, Elaine and Barthet, Mathieu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Affective Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1442--1460}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TAFFC.2021.3093787}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI21_sEMGPerformance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI21_sEMGPerformance" class="col-sm-7"> <div class="title">Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI21_sEMGPerformance.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI21_sEMGPerformance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Salzburg, Austria}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '21}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3430524.3440641}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450382137}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3430524.3440641}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_NIME20_VocalsEMG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME20_VocalsEMG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME20_VocalsEMG" class="col-sm-7"> <div class="title">Surface Electromyography for Direct Vocal Control</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME20_VocalsEMG.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a new method for direct control using the voice via measurement of vocal muscular activation with surface electromyography (sEMG). Digital musical interfaces based on the voice have typically used indirect control, in which features extracted from audio signals control the parameters of sound generation, for example in audio to MIDI controllers. By contrast, focusing on the musculature of the singing voice allows direct muscular control, or alternatively, combined direct and indirect control in an augmented vocal instrument. In this way we aim to both preserve the intimate relationship a vocalist has with their instrument and key timbral and stylistic characteristics of the voice while expanding its sonic capabilities. This paper discusses other digital instruments which effectively utilise a combination of indirect and direct control as well as a history of controllers involving the voice. Subsequently, a new method of direct control from physiological aspects of singing through sEMG and its capabilities are discussed. Future developments of the system are further outlined along with usage in performance studies, interactive live vocal performance, and educational and practice tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME20_VocalsEMG</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Direct Vocal Control}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Birmingham City University}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Birmingham, UK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{458--463}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.4813475}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nime.org/proceedings/2020/nime2020_paper88.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Michon, Romain and Schroeder, Franziska}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/1nWLgQGNh0g}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_ISMIR19_FeltTension.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_ISMIR19_FeltTension.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_ISMIR19_FeltTension" class="col-sm-7"> <div class="title">Effects of Musical Stimulus Habituation and Musical Training on Felt Tension</div> <div class="author"> <em>Courtney N. Reed</em>, and Elaine Chew</div> <div class="periodical"> <em>In Late Breaking/Demo at the Twentieth International Society for Music Information Retrieval Conference (ISMIR)</em>, Nov 2019 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_ISMIR19_FeltTension.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>As a listener habituates to a stimulus, its impact is expected to decrease over time; this research investigates the impact of repetitions and time on felt tension. Farbood describes tension increase as “a feeling of rising intensity or impending climax” and decrease as “a feeling of relaxation or resolution.” Musical tension has been linked to structural properties of music such as chord movements, tonality, and section boundaries; these connections have in turn informed the design of quantitative models for musical tension. In a pilot study, 9 participants annotated their felt tension through a recorded live performance of Chopin’s Ballade No. 2 in F Maj, Op. 38, and a collage of the ballade from the Arrhythmia Suite by Chew et al. The piece includes a calm triplet motif and a tense foil with frequent dissonance and variable features. The difference in felt tension over time for musicians and non-musicians is found to be significant at 5%. In a comparison of means t-test, H0 : μ1 = μ2 is rejected at 7 DOF (t = −2.580, p = 0.0365). In musicians, the range of annotated values is greater (68.46, non-musicians = 61.75), while mean tension over time is lower (22.33, non-musicians = 35.24), suggesting that musicians are more aware of the full emotional range of their felt tension and supports the heightened response of musicians to different musical stimuli.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_ISMIR19_FeltTension</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Effects of Musical Stimulus Habituation and Musical Training on Felt Tension}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Chew, Elaine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Late Breaking/Demo at the Twentieth International Society for Music Information Retrieval Conference (ISMIR)}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Delft, The Netherlands}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Reed_MSc_Changepoints.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_MSc_Changepoints.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_MSc_Changepoints" class="col-sm-7"> <div class="title">Interactions between felt emotion and musical change points in live music performance</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>MSc Computer Science, Queen Mary University of London</em>, Aug 2018 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>MSc Thesis</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_MSc_Changepoints.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This thesis in music cognition focuses on investigations of the felt emotions that arise through a listener‘s response to musical change. The research conducted aims to connect felt emotional response to tension and pleasure in musical signatures, with a focus on qualitative categorization of participant responses. Participants indicated their felt emotional responses to three recorded pieces of music previously performed for them in a live setting. The focus of the research is on felt emotion, which is relatively unexplored compared to perceived emotion. Participants annotated their overall emotion (in valence-arousal dimensions) and tension through the music. They also annotated where they felt transition points and sudden changes to the overall emotional quality of the piece occurred. Interactions between felt emotions and musical features, including loudness, tempo, and harmonic and melodic tension, were defined over the length of the performance in both a pedagogical examination and quantitative analysis with the use of music information retrieval software. The conclusions of this study will provide a basis for future music cognition study, especially in medical research of electrophysiological effects of mental stress on the heart and brain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">Reed_MSc_Changepoints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Interactions between felt emotion and musical change points in live music performance}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{MSc Computer Science, Queen Mary University of London}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Courtney N. Reed. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 01, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>