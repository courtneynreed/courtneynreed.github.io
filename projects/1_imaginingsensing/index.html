<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>imagining &amp; sensing | Courtney N. Reed</title> <meta name="author" content="Courtney N. Reed"> <meta name="description" content="my PhD work surrounding the vocalist-voice relationship, vocal embodiment and perception"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://courtneynreed.github.io/projects/1_imaginingsensing/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>body{background-image:url('/assets/img/bg-light.png');background-repeat:no-repeat;background-attachment:fixed;background-size:cover}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Courtney¬†</span>N.¬†Reed</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">papers</a> </li> <li class="nav-item"><a class="nav-link" href="https://docs.google.com/document/d/1zmnmAj1ok-t0i4b_8pbCEB78QDXGkr-yNywraocT8vM/edit?usp=sharing" rel="external nofollow noopener" target="_blank">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="background-color:white;"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">imagining &amp; sensing</h1> <p class="post-description">my PhD work surrounding the vocalist-voice relationship, vocal embodiment and perception</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/1-header-andrea-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/1-header-andrea-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/1-header-andrea-1400.webp"></source> <img src="/assets/img/proj-imagining/1-header-andrea.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>2019 - 2023</strong></p> <p>My PhD at Queen Mary University of London (QMUL) was done in the <a href="http://instrumentslab.org/" rel="external nofollow noopener" target="_blank">Augmented Instruments Lab (AIL)</a> with Prof. Andrew McPherson. The work culmuninated in my thesis, <a href="https://www.courtneynreed.com/assets/pdf/Reed_PhD_ImaginingSensing.pdf" rel="external nofollow noopener" target="_blank">Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback</a>.</p> <p>üèÜ This PhD was awarded the <a href="https://www.courtneynreed.com/blog/2024/dissertationaward/" rel="external nofollow noopener" target="_blank">ACM SIGCHI Outstanding Dissertation Award</a> in 2024 for contributions to the HCI community.</p> <h2 id="abstract">abstract</h2> <p>The voice is body, instrument, and identity: To explore and unpack the relationship that vocalists have with their voices and vocal practice, this work involved autoethnographic research in my own singing and work with other vocalists and practices.</p> <p>This work also explores the tensions and opportunities between third-person view of the voice by listeners, sensors, and digital agents, and the critical, embodied, first-person relationship of the vocalist. The vocalist‚Äôs understanding of their multi-sensory experiences is through tacit knowledge of the body. This knowledge is difficult to articulate, yet awareness and control of the body are innate. In the ever-increasing emergence of technology which quantifies or interprets physiological processes, we must remain conscious also of embodiment and human perception of these processes. Focusing on the vocalist-voice relationship, this project expands knowledge of human interaction and how technology influences our perception of our bodies. <br></p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <br><br> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/a-emgsetup-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/a-emgsetup-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/a-emgsetup-1400.webp"></source> <img src="/assets/img/proj-imagining/a-emgsetup.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/a-voxemg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/a-voxemg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/a-voxemg-1400.webp"></source> <img src="/assets/img/proj-imagining/a-voxemg.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Designing a vocal electromyography instrument, the VoxEMG. The original prototype worn here on my suprahyoid (left) with wired electrodes, conductive paste, and kinetic tape. The current PCB (right) is designed for textile incorporation and fabric electrode connections with castellated inputs. </div> <h2 id="contributions">contributions</h2> <ul> <li> <a href="https://www.courtneynreed.com/projects/2_voxemg/" rel="external nofollow noopener" target="_blank">VoxEMG electromyography platform</a>: a novel vocal interaction method which uses measurement of laryngeal muscular activations through surface electromyography (sEMG).</li> <li>Long-term first-person/autoethnographic and in-depth work with other vocalists on reflection on the incorporation of biosignal feedback understanding body movements and vocal practice and how such feedback can function as a metaphor.</li> <li>Comprehensive examination of how technology and the feedback we receive in human-computer interaction (HCI) can shape our perception and understanding of our bodies and our actions.</li> <li>Strategy for adopting technologies from other practices into traditional arts and other contexts through the use of soft wearables, e.g., the <a href="http://localhost:4000/assets/pdf/Reed_AHs22_SingingKnit.pdf" rel="external nofollow noopener" target="_blank">Singing Knit EMG wearable</a>.</li> <li>Analysis of how metaphors used in fundamental vocal pedagogy and how metaphorical communication between humans works; this proposed novel ways in which we can structure interaction with technology to aid in sensory communication.</li> <li>Biofeedback-based reflections on ways in which vocalists are in control and controlled by their voices, work with and against their bodies.</li> <li>Nuanced account of human interaction and perception of the body through vocal practice, as an example of how technological intervention enables exploration and influence over embodied understanding. <br><br> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/b-knitdesign-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/b-knitdesign-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/b-knitdesign-1400.webp"></source> <img src="/assets/img/proj-imagining/b-knitdesign.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/b-voxemg-knit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/b-voxemg-knit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/b-voxemg-knit-1400.webp"></source> <img src="/assets/img/proj-imagining/b-voxemg-knit.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/b-knit-stretch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/b-knit-stretch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/b-knit-stretch-1400.webp"></source> <img src="/assets/img/proj-imagining/b-knit-stretch.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/b-knit-stretch2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/b-knit-stretch2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/b-knit-stretch2-1400.webp"></source> <img src="/assets/img/proj-imagining/b-knit-stretch2.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Implementing the VoxEMG into the Singing Knit with designs by Sophie Skach (top). The VoxEMG board can be affixed to textiles and use conductive thread inputs (left); the conductive thread traces to fabric electrodes are woven into the knit (centre), which stretches with the garment (right). </div> <h2 id="theory--methods">theory &amp; methods</h2> <ul> <li>mixed-methods</li> <li>electromyography (EMG) biofeedback</li> <li>entanglement theory</li> <li>agential realism</li> <li>somaesthetics and soma design</li> <li>cognitive science and music psychology</li> <li>first-person methods &amp; autoethnography</li> <li>micro-phenomenology</li> <li>thematic analysis</li> <li>contemporary metaphor theory</li> <li>vocal organology <br><br> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/c-collar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/c-collar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/c-collar-1400.webp"></source> <img src="/assets/img/proj-imagining/c-collar.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-imagining/c-collarworn-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-imagining/c-collarworn-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-imagining/c-collarworn-1400.webp"></source> <img src="/assets/img/proj-imagining/c-collarworn.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Singing Knit inside with conductive fabric electrode pads (top left) and outside with affixed VoxEMG boards for 8 channels of EMG data (top right). As modelled by me, the garment is able to stretch for flexible performance wear (bottom row). </div> <h2 id="related-publications">related publications:</h2> </article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_MSX_AAFVocalAccuracy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_MSX_AAFVocalAccuracy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_MSX_AAFVocalAccuracy-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_MSX_AAFVocalAccuracy.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_MSX_AAFVocalAccuracy.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_MSX_AAFVocalAccuracy" class="col-sm-7"> <div class="title">Auditory imagery ability influences accuracy when singing with altered auditory feedback</div> <div class="author"> <em>Courtney N. Reed</em>,¬†Marcus Pearce,¬†and¬†Andrew McPherson</div> <div class="periodical"> <em>Musicae Scientiae</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-danger p-2" disabled>Article</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_MSX_AAFVocalAccuracy.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this preliminary study, we explored the relationship between auditory imagery ability and the maintenance of tonal and temporal accuracy when singing and audiating with altered auditory feedback (AAF). Actively performing participants sang and audiated (sang mentally but not aloud) a self-selected piece in AAF conditions, including upward pitch-shifts and delayed auditory feedback (DAF), and with speech distraction. Participants with higher self-reported scores on the Bucknell Auditory Imagery Scale (BAIS) produced a tonal reference that was less disrupted by pitch shifts and speech distraction than musicians with lower scores. However, there was no observed effect of BAIS score on temporal deviation when singing with DAF. Auditory imagery ability was not related to the experience of having studied music theory formally, but was significantly related to the experience of performing. The significant effect of auditory imagery ability on tonal reference deviation remained even after partialling out the effect of experience of performing. The results indicate that auditory imagery ability plays a key role in maintaining an internal tonal center during singing but has at most a weak effect on temporal consistency. In this article, we outline future directions in understanding the multifaceted role of auditory imagery ability in singers‚Äô accuracy and expression.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Reed_MSX_AAFVocalAccuracy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Auditory imagery ability influences accuracy when singing with altered auditory feedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Pearce, Marcus and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Musicae Scientiae}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SAGE Publications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{478‚Äì501}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/10298649231223077}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2045-4147}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.1177/10298649231223077}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI23_VocalMetaphor.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_VocalMetaphor.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_VocalMetaphor" class="col-sm-7"> <div class="title">Negotiating Experience and Communicating Information Through Abstract Metaphor</div> <div class="author"> <em>Courtney N. Reed</em>,¬†Paul Strohmeier,¬†and¬†Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_VocalMetaphor.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>An implicit assumption in metaphor use is that it requires grounding in a familiar concept, prominently seen in the popular Desktop Metaphor. In human-to-human communication, however, abstract metaphors, without such grounding, are often used with great success. To understand when and why metaphors work, we present a case study of metaphor use in voice teaching. Voice educators must teach about subjective, sensory experiences and rely on abstract metaphor to express information about unseen and intangible processes inside the body. We present a thematic analysis of metaphor use by 12 voice teachers. We found that metaphor works not because of strong grounding in the familiar, but because of its ambiguity and flexibility, allowing shared understanding between individual lived experiences. We summarise our findings in a model of metaphor-based communication. This model can be used as an analysis tool within the existing taxonomies of metaphor in user interaction for better understanding why metaphor works in HCI. It can also be used as a design resource for thinking about metaphor use and abstracting metaphor strategies from both novel and existing designs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_VocalMetaphor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Negotiating Experience and Communicating Information Through Abstract Metaphor}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3544548.3580700}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394215}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3544548.3580700}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI23_BodyLutherie-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI23_BodyLutherie.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI23_BodyLutherie.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI23_BodyLutherie" class="col-sm-7"> <div class="title">As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Body X Materials</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI23_BodyLutherie.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through soma-centric research, we see the different interaction roles of our bodies: they are the locus of our experience, a conduit for our expression and engagement, a sensor of feedback in the world, and a collaborator in our interaction with it. More" traditional" examinations of the body might look at control over it; for instance, in my research around vocal embodiment, I see many teachers and practitioners alike talking about how we can maintain control over the body. However, bodies are living, inconsistent, and typically weird. In reality, we do not have as much control over them as we would like or think we do. In this position paper, I will touch on my research around vocal physiology and sonified and vibrotactile feedback as I frame our role in a new light‚Äîdesigners as Body Luthiers, who must address the body as a material with inconsistencies, flaws, and variability, and work with it as a partner, embracing its uniqueness and changeability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI23_BodyLutherie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{As the Luthiers Do: Designing with a Living, Growing, Changing Body-Material}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Body X Materials}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Hamburg, Germany}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI23_BodyAsSound-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI23_BodyAsSound.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI23_BodyAsSound.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI23_BodyAsSound" class="col-sm-7"> <div class="title">The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback</div> <div class="author"> <em>Courtney N. Reed</em>,¬†and¬†Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI23_BodyAsSound.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Multi-sensory experiences underpin embodiment, whether with the body itself or technological extensions of it. Vocalists experience intensely personal embodiment, as vocalisation has few outwardly visible effects and kinaesthetic sensations occur largely within the body, rather than through external touch. We explored this embodiment using a probe which sonified laryngeal muscular movements and provided novel auditory feedback to two vocalists over a month-long period. Somatic and micro-phenomenological approaches revealed that the vocalists understand their physiology through its sound, rather than awareness of the muscular actions themselves. The feedback shaped the vocalists‚Äô perceptions of their practice and revealed a desire for reassurance about exploration of one‚Äôs body when the body-as-sound understanding was disrupted. Vocalists experienced uncertainty and doubt without affirmation of perceived correctness. This research also suggests that technology is viewed as infallible and highlights expectations that exist about its ability to dictate success, even when we desire or intend to explore.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI23_BodyAsSound</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Warsaw, Poland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '23}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3569009.3572738}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450399777}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3569009.3572738}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_PhD_ImaginingSensing-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_PhD_ImaginingSensing.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_PhD_ImaginingSensing.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_PhD_ImaginingSensing" class="col-sm-7"> <div class="title">Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>PhD Computer Science, Queen Mary University of London</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-info p-2" disabled>PhD Thesis</button> <button type="button" class="btn btn-sm z-depth-0 btn-warning p-2" disabled>ACM SIGCHI Outstanding Dissertation</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_PhD_ImaginingSensing.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The voice is body and instrument. Third-person interpretation of the voice by listeners, vocal teachers, and digital agents is centred largely around audio feedback. For a vocalist, physical feedback from within the body provides an additional interaction. The vocalist‚Äôs understanding of their multi-sensory experiences is through tacit knowledge of the body. This knowledge is difficult to articulate, yet awareness and control of the body are innate. In the ever-increasing emergence of technology which quantifies or interprets physiological processes, we must remain conscious also of embodiment and human perception of these processes. Focusing on the vocalist-voice relationship, this thesis expands knowledge of human interaction and how technology influences our perception of our bodies. To unite these different perspectives in the vocal context, I draw on mixed methods from cognitive science, psychology, music information retrieval, and interactive system design. Objective methods such as vocal audio analysis provide a third-person observation. Subjective practices such as micro-phenomenology capture the experiential, first-person perspectives of the vocalists themselves. Quantitative-qualitative blend provides details not only on novel interaction, but also an understanding of how technology influences existing understanding of the body.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Reed_PhD_ImaginingSensing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Imagining &amp; Sensing: Understanding and Extending the Vocalist-Voice Relationship Through Biosignal Feedback}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{PhD Computer Science, Queen Mary University of London}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME22_Microphenomenology-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME22_Microphenomenology.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME22_Microphenomenology.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME22_Microphenomenology" class="col-sm-7"> <div class="title">Exploring Experiences with New Musical Instruments through Micro-phenomenology</div> <div class="author"> <em>Courtney N. Reed</em>,¬†Charlotte Nordmoen,¬†Andrea Martelloni, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Giacomo Lepri, Nicole Robson, Eevee Zayas-Garin, Kelsey Cotton, Lia Mice, Andrew McPherson' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME22_Microphenomenology.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five microphenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME22_Microphenomenology</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Exploring Experiences with New Musical Instruments through Micro-phenomenology}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{The University of Auckland, New Zealand}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{49}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21428/92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.21428%2F92fbeb44.b304e4b1}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/-Ket6l90S8I}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI22_CommunicatingBodies.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_CommunicatingBodies.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_CommunicatingBodies" class="col-sm-7"> <div class="title">Communicating Across Bodies in the Voice Lesson</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Tangible Interaction for Well-Being</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_CommunicatingBodies.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this position paper, I would like to introduce my research on vocalists and their relationships with their bodies, and how the use of haptic feedback can improve these connections and the way we communicate sensory experience. I use the voice lesson and vocal performance as an environment to understand more broadly how people perceive very refined movements which they feel internally. My research seeks to understand how we communicate these sensory experiences in human-to-human interaction and how we can augment or communicate sensory experience through technology. I examine perception of these experiences through different feedback modalities, namely auditory and haptic feedback. Providing new ways to communicate our sensory experiences can lead to improvements in understanding between two individuals (for instance teacher and student). In virtual singing lessons, where the majority of voice study is being done in early 2022, this is especially important, as many of the common ways of interacting with the voice have disappeared with the transition to online interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_CommunicatingBodies</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Communicating Across Bodies in the Voice Lesson}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Tangible Interaction for Well-Being}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_CHI22_SensorySketching-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_CHI22_SensorySketching.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_CHI22_SensorySketching.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_CHI22_SensorySketching" class="col-sm-7"> <div class="title">Sensory Sketching for Singers</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In ACM CHI Workshop on Sketching Across the Senses</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_CHI22_SensorySketching.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This position paper outlines my study of vocalists and the relationships with the voice as both instrument and part of the body. I study this embodiment through a phenomenological perspective, employing somaesthetics and micro-phenomenology to explore the tacit relationships that singers have with their body. While verbal metaphor is traditionally used to articulate experience in teaching voice, I also use body mapping and material speculation to help articulate tactile and auditory experiences while singing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_CHI22_SensorySketching</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Sensory Sketching for Singers}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{ACM CHI Workshop on Sketching Across the Senses}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, LA, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_AHs22_SingingKnit.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_AHs22_SingingKnit.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_AHs22_SingingKnit" class="col-sm-7"> <div class="title">Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances</div> <div class="author"> <em>Courtney N. Reed</em>,¬†Sophie Skach,¬†Paul Strohmeier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew P. McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2022</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_AHs22_SingingKnit.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer‚Äôs vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_AHs22_SingingKnit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Skach, Sophie and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2022}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kashiwa, Chiba, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{170‚Äì183}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3519391.3519412}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450396325}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3519391.3519412}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI22_EmbodiedSingingDC.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI22_EmbodiedSingingDC.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI22_EmbodiedSingingDC" class="col-sm-7"> <div class="title">Examining Embodied Sensation and Perception in Singing</div> <div class="author"> <em>Courtney N. Reed</em> </div> <div class="periodical"> <em>In Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2022 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-success p-2" disabled>Extended Abstract</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI22_EmbodiedSingingDC.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces my PhD research on the relationship which vocalists have with their voice. The voice, both instrument and body, provides a unique perspective to examine embodied practice. The interaction with the voice is largely without a physical interface and it is difficult to describe the sensation of singing; however, voice pedagogy has been successful at using metaphor to communicate sensory experience between student and teacher. I examine the voice through several different perspectives, including experiential, physiological, and communicative interactions, and explore how we convey sensations in voice pedagogy and how perception of the body is shaped through experience living in it. Further, through externalising internal movement using sonified surface electromyography, I aim to give presence to aspects of vocal movement which have become subconscious or automatic. The findings of this PhD will provide understanding of how we perceive the experience of living within the body and perform through using the body as an instrument.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI22_EmbodiedSingingDC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Examining Embodied Sensation and Perception in Singing}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Daejeon, Republic of Korea}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '22}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3490149.3503581}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450391474}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3490149.3503581}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI21_sEMGPerformance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI21_sEMGPerformance" class="col-sm-7"> <div class="title">Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists</div> <div class="author"> <em>Courtney N. Reed</em>,¬†and¬†Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI21_sEMGPerformance.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI21_sEMGPerformance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Salzburg, Austria}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '21}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3430524.3440641}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450382137}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3430524.3440641}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME20_VocalsEMG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME20_VocalsEMG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME20_VocalsEMG" class="col-sm-7"> <div class="title">Surface Electromyography for Direct Vocal Control</div> <div class="author"> <em>Courtney N. Reed</em>,¬†and¬†Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="row"> ‚ÄÇ<button type="button" class="btn btn-sm z-depth-0 btn-primary p-2 text-left" disabled>Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME20_VocalsEMG.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a new method for direct control using the voice via measurement of vocal muscular activation with surface electromyography (sEMG). Digital musical interfaces based on the voice have typically used indirect control, in which features extracted from audio signals control the parameters of sound generation, for example in audio to MIDI controllers. By contrast, focusing on the musculature of the singing voice allows direct muscular control, or alternatively, combined direct and indirect control in an augmented vocal instrument. In this way we aim to both preserve the intimate relationship a vocalist has with their instrument and key timbral and stylistic characteristics of the voice while expanding its sonic capabilities. This paper discusses other digital instruments which effectively utilise a combination of indirect and direct control as well as a history of controllers involving the voice. Subsequently, a new method of direct control from physiological aspects of singing through sEMG and its capabilities are discussed. Future developments of the system are further outlined along with usage in performance studies, interactive live vocal performance, and educational and practice tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME20_VocalsEMG</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Direct Vocal Control}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Birmingham City University}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Birmingham, UK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{458--463}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.4813475}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nime.org/proceedings/2020/nime2020_paper88.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Michon, Romain and Schroeder, Franziska}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/1nWLgQGNh0g}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Courtney N. Reed. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>