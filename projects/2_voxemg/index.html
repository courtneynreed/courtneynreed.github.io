<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>voxemg | Courtney N. Reed</title> <meta name="author" content="Courtney N. Reed"> <meta name="description" content="a PCB pre-amplifier for vocal sEMG signals, designed for the small laryngeal muscles"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://courtneynreed.github.io/projects/2_voxemg/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>body{background-image:url('/assets/img/bg-light.png');background-repeat:no-repeat;background-attachment:fixed;background-size:cover}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Courtney </span>N. Reed</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">papers</a> </li> <li class="nav-item"><a class="nav-link" href="https://docs.google.com/document/d/1zmnmAj1ok-t0i4b_8pbCEB78QDXGkr-yNywraocT8vM/edit?usp=sharing" rel="external nofollow noopener" target="_blank">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" style="background-color:white;"> <div class="post"> <header class="post-header"> <h1 class="post-title">voxemg</h1> <p class="post-description">a PCB pre-amplifier for vocal sEMG signals, designed for the small laryngeal muscles</p> </header> <article> <div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center"> <div class="repo p-2 text-center"> <a href="https://github.com/courtcourtaney/voxEMG" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="courtcourtaney/voxEMG" src="https://github-readme-stats.vercel.app/api/pin/?username=courtcourtaney&amp;repo=voxEMG&amp;theme=default&amp;show_owner=true"> <img class="repo-img-dark w-100" alt="courtcourtaney/voxEMG" src="https://github-readme-stats.vercel.app/api/pin/?username=courtcourtaney&amp;repo=voxEMG&amp;theme=dark&amp;show_owner=true"> </a> </div> </div> <p><br> The VoxEMG is fully open source and <a href="https://github.com/courtcourtaney/voxEMG" rel="external nofollow noopener" target="_blank">all schematics, CAD files, and instruction for implementation can be found on Github.</a></p> <div class="row"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-voxemg/voxemg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-voxemg/voxemg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-voxemg/voxemg-1400.webp"></source> <img src="/assets/img/proj-voxemg/voxemg.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-1400.webp"></source> <img src="/assets/img/proj-voxemg/VoxEMG-bela.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-voxemg/VoxEMG-bela-2-1400.webp"></source> <img src="/assets/img/proj-voxemg/VoxEMG-bela-2.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> PCB versions of the VoxEMG. The v3.1 eTextile Configuration (left) features castellated inputs for conductive thread input and loops for textile integration. The v3.1.2 Bela Mini Capelet (middle, right) slots directly into the A0 and A1 analogue inputs, power, and ground from the Bela Mini. </div> <h2 id="about">about</h2> <p>The VoxEMG circuit is an extension of the open-source <a href="advancertechnologies.com/p/muscle-sensor-emg-circuitkit-bronze.html">EMG Circuit v7.1 (Advancer Technologies)</a>, from which other EMG platforms such as the Myoware are derived. VoxEMG is specifically aimed to detect activation of the extrinsic laryngeal muscles in both vocalised and subvocalised singing. High-precision and trimmable resistors are used to ensure noise reduction and the circuit is flexible to be used with different types of electrodes for desired implementation. The EMG signals can then be used in a variety of manners, for instance to relay feedback about the singer’s movements during practice. <br></p> <p>There are currently two open-source versions of the VoxEMG available on GitHub. The PCB configurations use the same circuit implemented in different PCB setups:</p> <ul> <li> <a href="https://github.com/courtcourtaney/voxEMG?tab=readme-ov-file#v31---etextile-configuration" rel="external nofollow noopener" target="_blank">v3.1 - eTextile Configuration</a> - designed for textile integrations and conductive thread input</li> <li> <a href="https://github.com/courtcourtaney/voxEMG?tab=readme-ov-file#v312---bela-mini-capelet" rel="external nofollow noopener" target="_blank">v3.1.2 - Bela Mini Capelet</a> - designed for direct plug-and-play with the Bela Mini</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj-voxemg/workflow-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj-voxemg/workflow-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj-voxemg/workflow-1400.webp"></source> <img src="/assets/img/proj-voxemg/workflow.png" class="img-fluid rounded" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The VoxEMG signal flow diagram for each muscle input and output. </div> <h2 id="related-publications">related publications:</h2> </article> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_AHs22_SingingKnit-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_AHs22_SingingKnit.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_AHs22_SingingKnit.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_AHs22_SingingKnit" class="col-sm-7"> <div class="title">Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances</div> <div class="author"> <em>Courtney N. Reed</em>, Sophie Skach, Paul Strohmeier, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrew P. McPherson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2022</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_AHs22_SingingKnit.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer’s vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_AHs22_SingingKnit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and Skach, Sophie and Strohmeier, Paul and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Augmented Humans International Conference 2022}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kashiwa, Chiba, Japan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AHs '22}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{170–183}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3519391.3519412}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450396325}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3519391.3519412}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_TEI21_sEMGPerformance.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_TEI21_sEMGPerformance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_TEI21_sEMGPerformance" class="col-sm-7"> <div class="title">Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew P. McPherson</div> <div class="periodical"> <em>In Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_TEI21_sEMGPerformance.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_TEI21_sEMGPerformance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew P.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction}}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Salzburg, Austria}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '21}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3430524.3440641}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450382137}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3430524.3440641}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Reed_NIME20_VocalsEMG-1400.webp"></source> <img src="/assets/img/publication_preview/Reed_NIME20_VocalsEMG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Reed_NIME20_VocalsEMG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Reed_NIME20_VocalsEMG" class="col-sm-7"> <div class="title">Surface Electromyography for Direct Vocal Control</div> <div class="author"> <em>Courtney N. Reed</em>, and Andrew McPherson</div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="row">  <button type="button" class="btn btn-sm z-depth-0 btn-primary p-2" disabled>Paper Proceedings</button> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Reed_NIME20_VocalsEMG.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a new method for direct control using the voice via measurement of vocal muscular activation with surface electromyography (sEMG). Digital musical interfaces based on the voice have typically used indirect control, in which features extracted from audio signals control the parameters of sound generation, for example in audio to MIDI controllers. By contrast, focusing on the musculature of the singing voice allows direct muscular control, or alternatively, combined direct and indirect control in an augmented vocal instrument. In this way we aim to both preserve the intimate relationship a vocalist has with their instrument and key timbral and stylistic characteristics of the voice while expanding its sonic capabilities. This paper discusses other digital instruments which effectively utilise a combination of indirect and direct control as well as a history of controllers involving the voice. Subsequently, a new method of direct control from physiological aspects of singing through sEMG and its capabilities are discussed. Future developments of the system are further outlined along with usage in performance studies, interactive live vocal performance, and educational and practice tools.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Reed_NIME20_VocalsEMG</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Surface Electromyography for Direct Vocal Control}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Reed, Courtney N. and McPherson, Andrew}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{Proceedings of the International Conference on New Interfaces for Musical Expression}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Birmingham City University}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Birmingham, UK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{458--463}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5281/zenodo.4813475}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2220-4806}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.nime.org/proceedings/2020/nime2020_paper88.pdf}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Michon, Romain and Schroeder, Franziska}</span><span class="p">,</span>
  <span class="na">presentation-video</span> <span class="p">=</span> <span class="s">{https://youtu.be/1nWLgQGNh0g}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Courtney N. Reed. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>